<!--
SPDX-FileCopyrightText: 2009 Fermi Research Alliance, LLC
SPDX-License-Identifier: Apache-2.0
-->

<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<html>
  <head>
    <meta http-equiv="CONTENT-TYPE" content="text/html; charset=UTF-8" />
    <title>GlideinWMS - Factory</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="../common/glideinWMS.css"
      media="screen, projection"
    />
    <link
      rel="canonical"
      href="https://glideinwms.fnal.gov/doc.prd/factory/custom_vars.html"
    />
  </head>

  <body lang="en-US" dir="ltr">
    <h1>
      <a href="index.html">GlideinWMS</a>
      <span>The Glidein-based Workflow Management System</span>
    </h1>
    <ul class="breadcrumbs">
      <li><a href="../index.html">Home</a></li>
      <li><a href="./index.html">WMS Factory</a></li>
      <li><a href="./configuration.html">Configuration</a></li>
      <li>Custom Variables</li>
    </ul>
    <div class="clear" />
    <div class="leftmenu">
      <ul class="components">
        <li><a href="../index.html">Home</a></li>
        <li><a href="../download.html">Download</a></li>
        <li><a href="../frontend/index.html">Glidein Frontend</a></li>
        <li><a href="../factory/index.html">WMS Factory</a></li>
        <li><a href="../components/index.html">Components</a></li>
        <li><a href="../recipes/index.html">Recipes</a></li>
        <li><a href="../components/faq.html" class="last">FAQ</a></li>
      </ul>
      <div class="search">
        <script>
          (function () {
            var cx = "013439253731257915088:h-xvmglqvrq";
            var gcse = document.createElement("script");
            gcse.type = "text/javascript";
            gcse.async = true;
            gcse.src = "https://cse.google.com/cse.js?cx=" + cx;
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(gcse, s);
          })();
        </script>
        <gcse:search enableAutoComplete="true"></gcse:search>
      </div>
    </div>
    <div class="content">
      <div class="heading">
        <img
          align="right"
          width="280px"
          border="0px"
          src="../images/simple_diagram.png"
          usemap="#rightimage"
        />
        <map name="rightimage">
          <area
            shape="rect"
            coords="90,3,177,60"
            href="../frontend/index.html"
          />
          <area
            shape="rect"
            coords="5,88,118,146"
            href="../components/user_pool.html"
          />
          <area
            shape="rect"
            coords="134,88,275,146"
            href="../factory/index.html"
          />
        </map>

        <h2>WMS Factory</h2>
        <ul class="subcomponents">
          <li><a href="./index.html">Overview</a></li>
          <li><a href="./details.html">Details</a></li>
          <li>Configuration</li>
          <li><a href="./design.html">Design</a></li>
          <li><a href="./monitoring.html">Monitoring</a></li>
          <li class="last">
            <a href="./troubleshooting.html">Troubleshooting</a>
          </li>
        </ul>
        <h3>Custom HTCondor Variables</h3>
      </div>
      <div class="jump">
        <u>Jump to:</u>
        <ol>
          <li><a href="#location">Variable location</a></li>
          <li><a href="#condor_vars">HTCondor vars file</a></li>
          <li><a href="#vars">Variables used</a></li>
          <li><a href="#admin_vars">Admin variables</a></li>
          <li><a href="#dyn_vars">Dynamic variables </a></li>
          <li>
            <a href="#singularity_vars">Apptainer/Singularity variables </a>
          </li>
          <li>
            <a href="#cvmfs_vars">CVMFS variables </a>
          </li>
          <li>
            <a href="#cpuscalculation">Set and discover available CPUs</a>
          </li>
          <li><a href="#lifetime">Lifetime of a glidein</a></li>
          <li><a href="#draining">Draining glideins</a></li>
        </ol>
      </div>
      <div class="related">
        Related Links: <br />
        <ul>
          <li><a href="./custom_scripts.html">Custom Scripts</a></li>
          <li><a href="./configuration.html">Factory configuration</a></li>
          <li>
            <a href="../frontend/configuration.html">Frontend configuration</a>
          </li>
        </ul>
      </div>
      <div class="section">
        <h2>Description</h2>
        <p>
          This document describes what configuration variables are used by the
          glideins. Most administrators never need to touch most of them, but a
          sophisticated Glidein Factory administrator may need to tweak some of
          them to implement the desired policies (for example: require
          encryption over the wire) or to address the needs of a particular site
          (for example: max allowed wallclock time).
        </p>
      </div>

      <div class="section">
        <h2><a name="location"></a>Configuration variable location</h2>
        <p>
          The GlideinWMS ships with a set of pre-defined configuration
          variables, that are stored in two files (known as
          <i>condor vars files)</i>:
        </p>

        <blockquote>
          glideinWMS/creation/web_base/condor_vars.lst<br />
          glideinWMS/creation/web_base/condor_vars.lst.entry
        </blockquote>
        <p>
          The two files are equivalent, but were split for historical reasons,
          and the second one is meant to contain site specific configuration
          variables.<br /><b
            >These files should never be modified, and represent just the
            default shipped by the software!</b
          >
        </p>
        <p>
          A GlideinWMS administrator can change the values of the predefined
          variables (with some exceptions, see <a href="#dyn_vars">below</a>),
          and define new ones using the
          <a href="configuration.html#args">Glidein Factory configuration</a>
          file.
        </p>
      </div>

      <div class="section">
        <h2><a name="condor_vars"></a>HTCondor vars files</h2>
        <p>
          The condor vars files contain the GlideinWMS pre-defined configuration
          variables, and <b>should never be modified</b>.<br />However, a
          GlideinWMS administrator should nevertheless be able to read them.
        </p>
        <p>
          Each of them is an ASCII file, with one entry per row. <br />Lines
          starting with # are comments and are ignored.
        </p>
        <p>
          Each non comment line must have 7 columns. Each column has a specific
          meaning:
        </p>
        <ol>
          <li>Attribute name</li>

          <li>
            Attribute type
            <ul>
              <li>I (int) &ndash; integer</li>
              <li>S (string) &ndash; quoted string</li>
              <li>
                C (expr) &ndash; unquoted string (i.e. HTCondor keyword or
                expression)
              </li>
            </ul>
          </li>
          <li>Default value, use &ndash; if no default</li>

          <li>
            HTCondor name, i.e. under which name should this attributed be known
            in the configuration used by HTCondor daemons
          </li>
          <li>
            Is a value required for this attribute? <br />Must be Y or N. If Y
            and the attribute is not defined, the glidein will fail.
          </li>
          <li>
            Will condor_startd publish this attribute to the collector?<br />Must
            be Y or N.
          </li>
          <li>
            Will the attribute be exported to the user job environment?
            <ul>
              <li>- - Do not export (for glidein/condor internal use)</li>

              <li>
                + - Export to the user job environment using the original
                attribute name
              </li>
              <li>
                @ - Export to the user job environment using the HTCondor name
              </li>
            </ul>
          </li>
        </ol>
        <p>
          Here below, you can see a short extract of an example; the semantics
          of the variables is defined <a href="#vars">below</a>.
        </p>
        <pre>
# VarName               Type    Default         CondorName                      Req.    Export  UserJobEnvName
#                       S=Quote - = No Default  + = VarName                             HTCondor  - = Do not export
#                                                                                               + = Use VarName
#                                                                                               @ = Use CondorName
#################################################################################################################
X509_USER_PROXY         C       -               GSI_DAEMON_PROXY                Y       N       -
USE_MATCH_AUTH          C       -     SEC_ENABLE_MATCH_PASSWORD_AUTHENTICATION  N       N       -
GLIDEIN_Factory         S       -               +                               Y       Y       @
GLIDEIN_Name            S       -               +                               Y       Y       @
GLIDEIN_Collector       C       -               HEAD_NODE                       Y       N       -
GLIDEIN_Expose_Grid_Env C       False     JOB_INHERITS_STARTER_ENVIRONMENT      N       Y       +
TMP_DIR                 S       -               GLIDEIN_Tmp_Dir                 Y       Y       @
CONDORG_CLUSTER         I       -               GLIDEIN_ClusterId               Y       Y       @
CONDORG_SUBCLUSTER      I       -               GLIDEIN_ProcId                  Y       Y       @
CONDORG_SCHEDD          S       -               GLIDEIN_Schedd                  Y       Y       @
SEC_DEFAULT_ENCRYPTION  C       OPTIONAL        +                               N       N       -
SEC_DEFAULT_INTEGRITY   C       REQUIRED        +                               N       N       -
MAX_MASTER_LOG          I       1000000         +                               N       N       -
MAX_STARTD_LOG          I       10000000        +                               N       N       -</pre
        >

        <b>NOTE The value in the sample are just an example.</b> The default
        values will depend on the GlideinWMS version, see
        <a
          href="https://raw.githubusercontent.com/glideinWMS/glideinwms/branch_v3_7/creation/web_base/condor_vars.lst"
          >creation/web_base/condor_vars.lst</a
        >
        and keep in mind that the values in the Factory and Frontend
        configuration and the changes done by the setup scripts will affect the
        final value used for your jobs.
      </div>

      <div class="section">
        <h2><a name="vars"></a>Glidein Variables</h2>
        <p>
          This section defines all the variables that the glideins explicitly
          use. Please be aware that, apart from the below mentioned variable
          many other variables will be used by the HTCondor daemons, since
          glideins are HTCondor based; see the
          <a
            target="_blank"
            href="https://htcondor.readthedocs.io/en/latest/admin-manual/introduction-to-configuration.html?highlight=ntroduction%20to%20Configuration#introduction-to-configuration"
            >HTCondor manual</a
          >
          for more details.
        </p>
        <p>The variables can be divided based on their source:</p>
        <ul>
          <li>
            <a href="#factory_attrs">Factory variables (config - attr tags)</a>
          </li>
          <li><a href="#factory_vars">Factory config - configuration</a></li>
          <li>
            <a href="#factory_frontend_attrs"
              >Factory or Frontend variables (config - attr tags)</a
            >
          </li>
          <li>
            <a href="#frontend_vars">Frontend variables (config - attr tags)</a>
          </li>
          <li><a href="#vm_vars">Cloud VM variables</a></li>
          <li><a href="#dyn_vars">Dynamically generated</a></li>
          <li><a href="#path_vars">Directory Paths</a></li>
          <li><a href="#glidein_vars">Glidein Scripts (dynamic)</a></li>
        </ul>
      </div>

      <div class="section">
        <h3>
          <a name="factory_attrs"></a>Factory variables (config - attr tags)
        </h3>
        <p>
          This section presents variables that can be directly changed by a
          Glidein Factory administrator using attr tags in the Factory
          configuration XML using the following tags:
        </p>
        <blockquote>
          <b>&lt;attr name</b>=&ldquo;<i>name</i>&rdquo;
          <b>value</b>=&ldquo;<i>val</i>&rdquo;

          <b>type</b>=&ldquo;<i>type</i>&rdquo; ...<b>/&gt;</b>
        </blockquote>
        <p>
          Attr tags can be in both the Factory and Frontend configuration,
          available types are <i>Int</i> (I in HTCondor vars file),
          <i>String</i> (S) or <i>Expr</i> (C). String and Expr values are
          treated literally (no need to escape them) and Strings are quoted when
          passed to HTCondor.
        </p>
        <p>
          More information on the XML format can be found in
          <a href="configuration.html#args">Glidein Factory configuration</a>
          section.
          <br />
          If not specified in the XML, most of these variables have defaults set
          in <i>condor vars</i> files, which are used if the Glidein Factory
          administrator does not override them. These defaults are listed below.
        </p>
        <p>
          Please also note that some of these variables may also be provided by
          the VO clients (e.g. Frontends). See the sections below.
        </p>

        <table class="attributes">
          <tr class="head">
            <td>
              <p><b>Name</b></p>
            </td>
            <td>
              <p><b>Type</b></p>
            </td>
            <td>
              <p><b>Default Value</b></p>
            </td>
            <td>
              <p><b>Description</b></p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_Site</b></td>
            <td>String</td>
            <td>Entry name</td>
            <td>
              <p>
                Logical name of the Grid site where the glidein is running. This
                information is published both in the startd ClassAd and in the
                user job environment.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_Hold</b></td>
            <td>Expr (Bool)</td>

            <td>True</td>
            <td>
              <p>
                HTCondor expression to use to specify when a user job in the
                glideins should be held. If any expression is true, the glidein
                is held. This is usually done to specify "bad" jobs, such as
                those that claim too much memory.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_Entry_PREEMPT</b></td>
            <td>Expr (Bool)</td>
            <td>True</td>
            <td rowspan="2">
              <p>
                HTCondor expression to use to specify when a user job in the
                glideins should be preempted. If any expression is true, the
                glidein is preempted. This is usually done to specify custom
                preemption policies for user jobs.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_PREEMPT</b></td>
            <td>Expr (Bool)</td>

            <td>True</td>
          </tr>

          <tr>
            <td><b>GLIDEIN_Rank</b></td>
            <td>Expr (Int)</td>
            <td>1</td>
            <td rowspan="2">
              <p>
                Used in calculating the HTCondor
                <a
                  href="https://htcondor.readthedocs.io/en/latest/admin-manual/configuration-macros.html?highlight=configuration%20macros#configuration-macros"
                  >RANK</a
                >
              </p>
              <p>
                They are summed together, and the user job with the largest rank
                will run first.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_Entry_Rank</b></td>
            <td>Expr (Int)</td>
            <td>1</td>
          </tr>

          <tr>
            <td><b>GLIDEIN_Max_Idle</b></td>
            <td>Int</td>
            <td>1200 <br />(20 mins)</td>
            <td>
              <p>
                Max amount of time a condor_startd will wait to be matched
                before giving up and terminating.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_Max_Tail</b></td>
            <td>Int</td>
            <td>400 <br />(6 mins)</td>
            <td>
              <p>
                Max amount of time a condor_startd will wait after having
                already completed a job to be matched again. (i.e. the tail of a
                job).
              </p>
            </td>
          </tr>

          <tr>
            <td><b>GLIDEIN_Retire_Time</b></td>
            <td>Int</td>
            <td>21600 <br />(6 hours)</td>
            <td rowspan="2">
              <p>
                How long the condor_startd be running before no longer accepting
                jobs.
                <br />
                The random spread is used to improve the efficiencies, so the
                actual value used by HTCondor will be anywhere between GRT-GRTS
                and GRT.
              </p>
              <p>
                NOTE: if GLIDEIN_Max_Walltime is specified, then this value is
                ignored and the retire time is calculated (see
                GLIDEIN_Max_Walltime below)
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_Retire_Time_Spread</b></td>
            <td>Int</td>
            <td>7200<br />(2 hours)</td>
          </tr>
          <tr>
            <td><b>GLIDEIN_Max_Walltime</b></td>
            <td>Int</td>
            <td>N/A</td>

            <td>
              <p>
                Max allowed time for the glidein.<br />This variable is
                optional. If you specify this variable, then HTCondor startup
                scripts will calculate the GLIDEIN_Retire_Time for the glidein
                as GLIDEIN_MAX_Walltime - GLIDEIN_Job_Max_Time. If
                GLIDEIN_Retire_Time is also specified, it will be ignored and
                only the calculated value is used. Note that if
                GLIDEIN_Job_Max_Time is specified but is greater than
                GLIDEIN_Max_Walltime or does not allow enough time for graceful
                shutdown, it will be reduced by the glidein. See
                <a href="#lifetime">Lifetime of a glidein</a> for details.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_Graceful_Shutdown</b></td>
            <td>Int</td>
            <td>120</td>
            <td>
              <p>
                Once DAEMON_SHUTDOWN is reached and the glidein pilot enters the
                <a
                  href="https://htcondor.readthedocs.io/en/latest/admin-manual/policy-configuration.html?highlight=Policy%20configuration#policy-configuration-for-execute-hosts-and-for-submit-hosts"
                  ><i>Retiring</i> state</a
                >, this amount passes to allow the startd and job to gracefully
                shutdown before forcefully terminating the glidein. See
                <a href="#lifetime">Lifetime of a glidein</a> for details.
              </p>
            </td>
          </tr>

          <tr>
            <td><b>PREEMPT</b></td>
            <td>Expr (Bool)</td>
            <td>False</td>
            <td>
              Specifies whether preemption is allowed to occur on this glidein.
              N.B.: Starting from 3.4.3, glideins in the Factory queues are
              removed 12 hours after they hit the walltime. This is done with
              periodic remove expression, and happens even if <b>PREEMPT</b> is
              false.
            </td>
          </tr>
          <tr>
            <td><b>PREEMPT_GRACE_TIME</b></td>
            <td>Int</td>
            <td>10000000</td>
            <td>
              This value affects the condor value "MaxJobRetirementtime" and
              it's an integer value representing the number of seconds a
              preempted job will be allowed to run before being evicted. This
              only affects behaviour if PREEMPT=True. After 12 hours, a glidein
              is removed from the factory queue anyway.
            </td>
          </tr>
          <tr>
            <td><b>HOLD_GRACE_TIME</b></td>
            <td>Int</td>
            <td>0</td>
            <td>
              This value affects the condor value "MaxJobRetirementtime" and is
              an integer value representing the number of seconds a job that
              triggers WANT_HOLD will be allowed to run before being evicted.
              This only affects behaviour if GLIDEIN_HOLD, GLIDEIN_Entry_HOLD,
              GLIDECLIENT_HOLD, or GLIDECLIENT_Group_HOLD are specified and
              become true. By default, these "bad" jobs are immediately evicted.
            </td>
          </tr>

          <tr>
            <td><b>GLIDEIN_Monitoring_Enabled</b></td>
            <td>Expr (Bool)</td>
            <td>True</td>
            <td>
              <p>
                Ability to control whether the pseduo-interactive monitoring
                slot is started on the worker node. Set to False if you do not
                want the monitoring slot started.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_Resource_Slots</b></td>
            <td>String</td>
            <td>None</td>
            <td>
              <p>
                Special purpose resources added in separate slots or the main
                slot of the glidein. The separate slots will never be available
                for the regular jobs. This string is a semicolon separated list
                of comma separated resource descriptions. Each resource
                description contains the <b>name</b> (case insensitive) and
                optionally the <b>number</b> of resource instances (default is 1
                (*)), the total <b>memory</b> reserved (default is 128MB times
                the number of resources), a <b>type</b> to control how the
                resource is handled (see below for more), and the
                <b>disk</b> reserved (default is <tt>auto</tt>, where HTCondor
                splits evenly). Possible type values are:
              </p>
              <ul>
                <li><b>main</b> - the resource is added to the main slot</li>
                <li>
                  <b>extra</b> - the resource is added to the main slot with
                  virtual CPUs
                </li>
                <li>
                  <b>partitionable</b> - the resource has a new dedicated slot
                  with virtual CPUs
                </li>
                <li>
                  <b>static</b> - the resource has many dedicated slots (one per
                  instance) with a virtual CPU each. Memory is taken from the
                  main partitionable slot.
                </li>
                <li>
                  <b>staticextra</b> - like <tt>static</tt>, but memory is
                  <i>added</i> to the main partitionable slot instead of
                  subtracted from it. Useful when you want dedicated static
                  slots that also increase the overall memory available to the
                  glidein. Memory defined in multiple <b>staticextra</b> slots
                  will cumulate.
                </li>
              </ul>
              <p>
                The default type is partitionable, unless there is only one
                resource instance, then is static. When adding resources to the
                main slots (type is <tt>main</tt> or <tt>extra</tt>): the memory
                and disk parameters are ignored; HTCondor splits automatically
                resources depending on the number of CPUs and having a
                partitionable main slot or not; if the number of resources is
                not equal (or an exact multiple) to the number of CPUs, then you
                must select partitionable slots for the Glideins (<tt
                  >slots_layout="partitionable"</tt
                >
                in the config/submit section of the entry configuration or
                SLOTS_LAYOUT partitionable in the Frontend configuration)
                otherwise the startd may fail due to impossible configuration.
                When adding resources to the main slots, regular jobs (not using
                the resource) may use all memory and CPUs (including the virtual
                ones). Only separate slots (type is <tt>partitionable</tt>,
                <tt>static</tt>, or <tt>staticextra</tt>) reserve CPUs and
                memory.
                <!--,
        e.g. when you add 3 units of a resource to a node with 8 CPUs divided statically in 8 main slots, the first 3 of them will have also your resource.
        -->
                Check the
                <a
                  href="https://htcondor.readthedocs.io/en/latest/admin-manual/policy-configuration.html?highlight=Policy%20configuration#policy-configuration-for-execute-hosts-and-for-submit-hosts"
                >
                  HTCondor manual to learn more on how resources are split</a
                >.
              </p>
              <p>
                The parameters in a resource description can be listed or
                specified using their name: name, number, memory, type, disk
                (see the last example below).
              </p>
              <p>
                Characters may be appended to a numerical value of memory to
                indicate percentage (%) or units. K or KB indicates KiB,
                $2^{10}$ numbers of bytes. M or MB indicates MiB, $2^{20}$
                numbers of bytes. G or GB indicates GiB, $2^{30}$ numbers of
                bytes. T or TB indicates TiB, $2^{40}$ numbers of bytes. Disk
                space must be a fraction or percentage: 1/4, 25%. auto lets
                HTCondor do the splitting. Check the
                <a
                  href="https://htcondor.readthedocs.io/en/latest/admin-manual/policy-configuration.html?highlight=Policy%20configuration#policy-configuration-for-execute-hosts-and-for-submit-hosts"
                  >HTCondor manual</a
                >
                for more about resource splitting.
              </p>
              <p>
                Jobs submission must use <tt>request_RESOURCE=N</tt> with
                <tt>N&gt;0</tt> to use these slots or to use the resource in the
                main slot, e.g. request_ioslot=1
              </p>
              <p>Examples:</p>
              <ul>
                <li>
                  <b>value="ioslot,2,200,static"</b> creates statically
                  partitioned 2 ioslot resources with 100MB memory each.
                </li>
                <li>
                  <b>value="ioslot,2,200,staticextra"</b> creates statically
                  partitioned 2 ioslot resources with 100MB memory each, and
                  this memory is added to the main slot instead of subtracted.
                </li>
                <li>
                  <b>value="ioslot"</b> creates 1 ioslot resource with 128MB
                </li>
                <li>
                  <b>value="ioslot,2,disk=1GB;monitor;GPUs,3,,main"</b> creates
                  (1) a partitionable slot with 2 ioslot resources, 256MB total
                  memory and 1GB total disk (2) 1 monitor slot resource with
                  128MB memory (3) adds 3 GPUs to the main slot/slots
                </li>
                <li>
                  <b>value="GPUs,type=main"</b> discovers (*) how many GPUs are
                  available and adds them to the main slot
                </li>
              </ul>
              <p>
                (*) GPUs is a special resource name. If no number is specified,
                the glidein will invoke
                <a
                  href="https://htcondor-wiki.cs.wisc.edu/index.cgi/wiki?p=HowToManageGpus"
                  >HTCondor's GPU discovery mechanism</a
                >
                and get the number from there, which could be 0 if there are no
                GPUs. Your job will find in the HTCondor ClassAd also all the
                special attributes about the GPUs. If no number is specified and
                the auto-discovery fails 0 GPUs are assumed. Since you don't
                know the number of GPUs that will be added to the main slot, the
                slot_layout must be partitionable. We do recommend to let
                HTCondor discover the number of GPUs by specifying no number. If
                you specify a number and is different from what HTCondor
                detects, there may be problems if you are using the HTCondor
                provided GPU description and monitoring.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_Use_PGroups</b></td>
            <td>Expr (Bool)</td>
            <td>False</td>

            <td>
              <p>Should process group monitoring be enabled?</p>
              <p>
                This is a
                <a
                  href="https://htcondor.readthedocs.io/en/latest/admin-manual/introduction-to-configuration.html?highlight=ntroduction%20to%20Configuration#introduction-to-configuration"
                  >HTCondor optimization</a
                >
                parameter. Unfortunately, it negatively interferes with the
                batch systems used by the Grid sites, so it should not be turned
                on unless you have a very good reason to do so.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>UPDATE_COLLECTOR_WITH_TCP</b></td>
            <td>Expr (Bool)</td>
            <td>True</td>
            <td>
              <p>
                If True, forces the glidein to use
                <a href="configuration.html#tcp">TCP updates</a>.<br />The
                collector must be configured in the same way for this to work.
              </p>

              <p>
                Also see the
                <a
                  href="https://htcondor.readthedocs.io/en/latest/admin-manual/configuration-macros.html?highlight=UPDATE_COLLECTOR_WITH_TCP#network-related-configuration-file-entries"
                  >HTCondor documentation</a
                >
                for implications and side effects.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>WANT_UDP_COMMAND_SOCKET</b></td>

            <td>Expr (Bool)</td>
            <td>False</td>
            <td>
              <p>
                If True, enable the startd UDP command socket (HTCondor
                default).
              </p>
              <p>
                Using the UDP command socket is a
                <a
                  href="https://htcondor.readthedocs.io/en/latest/admin-manual/configuration-macros.html?highlight=Network-Related%20Configuration%20File%20Entries#network-related-configuration-file-entries"
                  >HTCondor optimization</a
                >
                that makes working over firewalls and NATs very difficult. It is
                thus recommended you leave it disabled in the glideins.
              </p>

              <p>
                Please note if you leave it disabled, that you must configure
                the schedd with<br />SCHEDD_SEND_VACATE_VIA_TCP = True<br />and
                the negotiator with<br /><a
                  href="https://htcondor.readthedocs.io/en/latest/admin-manual/configuration-macros.html?highlight=Network-Related%20Configuration%20File%20Entries#condor-negotiator-configuration-file-entries"
                  >NEGOTIATOR_INFORM_STARTD</a
                >
                = False<br />to have a functional system.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>STARTD_SENDS_ALIVES</b></td>

            <td>Expr (Bool)</td>
            <td>True</td>
            <td>
              <p>
                If set to False, the schedd will be sending keepalives to the
                startd.
              </p>
              <p>
                Setting this to True instructs the startd to send keepalives to
                the schedd. This improves the glidein behavior when running
                behind a firewall or a NAT.
              </p>
              <p>
                Please note that the schedd must be configured in the same way
                for this to work.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>SEC_DEFAULT_INTEGRITY</b></td>
            <td>Expr</td>
            <td>REQUIRED</td>

            <td rowspan="2">
              <p>
                Security related settings. Please notice that the glideins
                always require GSI authentication.
              </p>
              <p>
                For more details see the
                <a href="configuration.html#security">configuration page</a> or
                the
                <a
                  href="https://htcondor.readthedocs.io/en/latest/admin-manual/security.html?highlight=Security#security"
                  >HTCondor manual</a
                >.
              </p>
            </td>
          </tr>

          <tr>
            <td><b>SEC_DEFAULT_ENCRYPTION</b></td>
            <td>Expr</td>
            <td>OPTIONAL</td>
          </tr>

          <tr>
            <td><b>USE_MATCH_AUTH</b></td>
            <td>Expr (Bool)</td>
            <td>True</td>
            <td>
              <p>Another security setting.</p>
              <p>
                If set to True (default), the schedd and the startd will use a
                low overhead protocol. See the
                <a href="configuration.html#security">configuration page</a> or
                the
                <a
                  href="https://htcondor.readthedocs.io/en/latest/admin-manual/configuration-macros.html?highlight=Configuration%20Macros"
                  >HTCondor manual</a
                >.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>MAX_MASTER_LOG</b></td>
            <td>Int</td>
            <td>1M</td>
            <td rowspan="3">
              <p>What is the maximum size the logs should grow.</p>

              <p>
                Setting them too low will made debugging difficult.<br />Setting
                them too high may fill up the disk in anomalous situations, both
                on the work nodes and on the glidein Factory.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>MAX_STARTD_LOG</b></td>
            <td>Int</td>

            <td>10M</td>
          </tr>
          <tr>
            <td><b>MAX_STARTER_LOG</b></td>
            <td>Int</td>

            <td>10M</td>
          </tr>
          <!--
    <TR>
        <td> <b>GCB_LIST</b> </td>
        <TD> Expr (List) </TD>

        <TD ROWSPAN=4> - </TD>
        <TD ROWSPAN=4>
            <P>GCB was needed to run glideins on worker nodes behind a
            firewall or a NAT in older versions of HTCondor.<BR>
            <B>Note: Newer versions of HTCondor do not support it anymore.</B></P>
            <P>For more information, see the <A HREF="../components/condor.html#gcb">configuration page</A>.</P>

        </TD>
    </TR>
    <TR>
        <td> <b>GCB_ORDER</b> </td>
        <TD> String </TD>
    </TR>

    <TR>
        <td> <b>GCB_MIN_FREE</b> </td>
        <TD> Int </TD>
    </TR>
    <TR>
        <td> <b>GCB_REMAP_ROUTE</b> </td>

        <TD> String </TD>
    </TR>
    <TR>
        <td> <b>MASTER_GCB_RECONNECT_TIMEOUT</b> </td>
        <td> Int </TD>

        <TD> 1200<BR>(20 mins) </TD>
        <TD>
            <P>Specifies how long should a glidein wait
            before giving up on a
            GCB, if network connectivity is lost.</P>
        </TD>
    </TR>
    -->
          <tr>
            <td><b>USE_CCB</b></td>

            <td>Expr (Bool)</td>
            <td>False</td>
            <td>
              <p>
                If set to True, it will enable
                <a
                  href="https://htcondor.readthedocs.io/en/latest/admin-manual/networking.html?highlight=CCB#networking-includes-sections-on-port-usage-and-ccb"
                  >HTCondor Connection Brokering (CCB)</a
                >, which is needed if the glideins are behind a firewall or a
                NAT.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>USE_SHARED_PORT</b></td>

            <td>Expr (Bool)</td>
            <td>False</td>
            <td>
              <p>
                If set to True, it will enable
                <a
                  href="https://htcondor.readthedocs.io/en/latest/admin-manual/networking.html?highlight=CCB#reducing-port-usage-with-the-condor-shared-port-daemon"
                  >the shared port daemon</a
                >, which will reduce the number of connections between the
                glidein and the collectors.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_CPUS</b></td>
            <td>String</td>

            <td>1</td>
            <td>
              <p>
                Number of CPUs glidein should use. GLIDEIN_CPUS is used to set
                NUM_CPUS for the HTCondor started by the glidein. Use
                "<b>slot</b>" (or "<b>auto</b>") to let glidein determine this
                from the job slot assigned to the glidein, use "<b>node</b>" to
                let glidein determine this from the hardware of the worker node
                (e.g. if you are sure that only your job is running on the
                node), use a positive integer to set GLIDEIN_CPUS (and NUM_CPUS)
                to assume that value. Most of the time it is an integer, but the
                type is string to allow also the keywords. In case of static
                partitioning, glidein will create GLIDEIN_CPUS number of slots.
                In case of dynamic partitioning, the slots will be created
                automatically based on the CPUs required by the job and
                GLIDEIN_CPUS is the sum of the slots made available. Refer to
                HTCondor manual for info on
                <a
                  href="https://htcondor.readthedocs.io/en/latest/admin-manual/configuration-macros.html?highlight=NUM_CPUS#condor-startd-configuration-file-macros"
                  >NUM_CPUS</a
                >
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_ESTIMATED_CPUS</b></td>
            <td>Int</td>

            <td>1</td>
            <td>
              <p>
                Number of CPUs glidein is expected to receive.
                GLIDEIN_ESTIMATED_CPUS is used only when GLIDEIN_CPUS is set to
                some auto-discovery value (auto/slot, node). With cores
                auto-discovery the Frontend assumes the entry will provide
                GLIDEIN_ESTIMATED_CPUS cores, 1 by default. The Frontend will
                work even if the estimate is not correct, anyway: it will
                trigger requests for the entry only if the CPUs requested by the
                jobs are no more than GLIDEIN_ESTIMATED_CPUS and, it will adjust
                the number of Glideins requested assuming GLIDEIN_ESTIMATED_CPUS
                cores from each Glidein in that entry. GLIDEIN_ESTIMATED_CPUS is
                not used by the Glidein. GLIDEIN_ESTIMATED_CPUS should be set
                only when GLIDEIN_CPUS is set to some auto-discovery value
                (auto/slot, node). It is a configuration error to set
                GLIDEIN_ESTIMATED_CPUS otherwise.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_OVERLOAD_ENABLED</b></td>
            <td>String (boolean or percentage)</td>
            <td>false</td>
            <td>
              <p>
                Controls whether resource overloading is enabled for the pilot.
                When set to
                <code>true</code> (case-insensitive), both
                <b>GLIDEIN_OVERLOAD_CPUS</b> and
                <b>GLIDEIN_OVERLOAD_MEMORY</b> adjustments are applied. When set
                to <code>false</code>, overloading is disabled regardless of the
                other overload values.
              </p>
              <p>
                If set to a percentage (e.g., <code>80%</code>), overloading is
                enabled randomly with the given probability. This is useful for
                gradual rollout or A/B testing. For example, setting
                <code>GLIDEIN_OVERLOAD_ENABLED=25%</code> enables overloading
                for approximately one in four pilots.
              </p>
              <p>
                This variable must be explicitly enabled to allow CPU or memory
                overload to take effect.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_OVERLOAD_CPUS</b></td>
            <td>Float</td>
            <td>1</td>
            <td>
              <p>
                Adjustment factor for <b>GLIDEIN_CPUS</b> that allows CPU
                overcommit for pilots. The value of <b>GLIDEIN_CPUS</b> will be
                multiplied by this factor to create a larger pilot slot than
                originally requested.
              </p>
              <p>
                For example, if <b>GLIDEIN_CPUS</b> and the CE submit attribute
                (e.g., <code>+xcount</code>) are both set to <code>8</code>, and
                <code>GLIDEIN_OVERLOAD_CPUS=1.25</code> is set, the pilot will
                report <b>GLIDEIN_CPUS=10</b> while still requesting only 8 from
                the CE.
              </p>
              <p>
                This attribute is required for whole-node pilots when the
                reported slot size differs from the CE submission size.
                Therefore, we recommend always using
                <b>GLIDEIN_OVERLOAD_CPUS</b> when <b>GLIDEIN_CPUS</b> differs
                from <code>+xcount</code>
                or other relevant CE attributes.
              </p>
              <p>
                Note: The overload is only applied if
                <b>GLIDEIN_OVERLOAD_ENABLED</b> is also enabled.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_OVERLOAD_MEMORY</b></td>
            <td>Float</td>
            <td>1</td>
            <td>
              <p>
                Adjustment factor for <b>GLIDEIN_MaxMemMBs</b> that allows
                memory overcommit for pilots. The value of
                <b>GLIDEIN_MaxMemMBs</b> will be multiplied by this factor to
                enlarge the pilot memory footprint beyond what the CE sees.
              </p>
              <p>
                For example, if both <b>GLIDEIN_MaxMemMBs</b> and the submit
                attribute <code>+MaxMemory</code> are set to
                <code>20Mb</code> (e.g., 2.5GB per core), and
                <code>GLIDEIN_OVERLOAD_MEMORY=1.25</code> is set, the pilot will
                report <b>GLIDEIN_MaxMemMBs=25Mb</b> while requesting only 20Mb.
              </p>
              <p>
                This attribute is required for whole-node pilots when the
                reported memory size differs from what is submitted to the CE.
                Therefore, we recommend always using
                <b>GLIDEIN_OVERLOAD_MEMORY</b> when
                <b>GLIDEIN_MaxMemMBs</b> differs from <code>+MaxMemory</code>
                or similar CE attributes.
              </p>
              <p>
                Note: The overload is only applied if
                <b>GLIDEIN_OVERLOAD_ENABLED</b> is also enabled.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_NODES</b></td>
            <td>Int</td>

            <td>1</td>
            <td>
              <p>
                Number of NODEs a Glidein is expected to be replicated on. On
                some entries, especially HPC resources, each Glidein submission
                triggers the startup of the Glidein on multiple nodes. The
                Frontend assumes that for each requested Glidein the entry will
                start it on GLIDEIN_NODES nodes, 1 by default. The Frontend will
                work even if the estimate is not correct, anyway: it will
                trigger requests for the entry only if the CPUs requested by the
                jobs are no more than GLIDEIN_ESTIMATED_CPUS and, it will adjust
                the number of Glideins requested assuming GLIDEIN_NODES
                Glideins, each with GLIDEIN_CPUS/GLIDEIN_ESTIMATED_CPUS cores
                for each request to that entry. GLIDEIN_NODES is not used by the
                Glidein.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_MaxMemMBs</b></td>
            <td>Int</td>
            <td>None</td>
            <td>
              <p>
                Amount of memory glidein should use. If set, GLIDEIN_MaxMemMBs
                is used to set total MEMORY used for the HTCondor Startd and
                jobs started by the glidein. If GLIDEIN_MaxMemMBs is not set and
                GLIDEIN_MaxMemMBs_Estimate is TRUE, GLIDEIN_MaxMemMBs is
                calculated based on the memory available on the worker node. If
                GLIDEIN_MaxMemMBs is not set and GLIDEIN_MaxMemMBs_Estimate is
                not TRUE (False or unset), the glidein lets HTCondor decide the
                amount of memory. Refer to HTCondor manual for info on
                <a
                  href="https://htcondor.readthedocs.io/en/latest/admin-manual/introduction-to-configuration.html#introduction-to-configuration"
                  >MEMORY</a
                >
              </p>
              <p>
                Note that HTCondor-CEs ignore RequestMemory. if you like to set
                the memory or change the default 2GB minimum, you need to set
                maxMemory, e.g.
                <tt>+maxMemory=RequestMemory</tt> in the submit_attrs section.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_MaxMemMBs_Estimate</b></td>
            <td>Expr (Bool)</td>

            <td>False</td>
            <td>
              <p>
                Used in conjunction with GLIDEIN_MaxMemMBs. It is ignored if
                GLIDEIN_MaxMemMBs is set to any value. See GLIDEIN_MaxMemMBs for
                the description.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_DISK</b></td>
            <td>Str</td>

            <td>EMPTY</td>
            <td>
              <p>
                Disk amount that the jobs running in the Glidein should use.
                This is used to configure the HTCondor SLOT_TYPE_ definitions
                that affect the value of Disk/TotalSlotDisk. Valid values are
                the ones accepted by HTCondor for the "disk=" keyword in the
                SLOT_TYPE_ definition. The default is an empty string
                (undefined) which is equivalent to "auto". It seems that for
                disk, valid values are the "auto" string (that lets HTCondor
                discover and handle the space), fractions or percentages, not
                absolute values. See the
                <a
                  href="https://htcondor.readthedocs.io/en/latest/admin-manual/policy-configuration.html#dividing-system-resources-in-multi-core-machines"
                  >HTCondor manual</a
                >.
              </p>
              <p>
                NOTE: this setting does not assure that a certain amount of disk
                space will be available. To do so you'll have to use
                request_disk in the submit_attr section
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_Factory_Report_Failed</b></td>
            <td>String</td>
            <td>ALIVEONLY</td>

            <td>
              <p>
                This attribute regulates advertising of validation failures to
                the Factory collector.
              </p>
              <ul>
                <li>
                  <b>NEVER</b> - The glidein will just fail, and do nothing.
                </li>
                <li>
                  <b>ALIVEONLY</b> - The glidein will advertise a Master ClassAd
                  in <i>Drained</i> state and <i>Retiring</i> activity for the
                  lifetime of the glidein script.
                </li>
                <li>
                  <b>ALWAYS</b> - Similar to <b>ALIVEONLY</b>, but will also
                  send a final ClassAd in <i>Drained</i> state and
                  <i>Killing</i> activity just before dying.
                </li>
              </ul>

              <p>
                When advertised, the classad is flagged
                <i>GLIDEIN_Failed=True</i>, the error is recorded in the
                <i>GLIDEIN_FAILURE_REASON</i> and
                <i>GLIDEIN_EXIT_CODE</i> attributes, and the failing script is
                recorded in the <i>GLIDEIN_LAST_SCRIPT</i> attribute.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_LOG_RECIPIENTS_FACTORY</b></td>
            <td>String</td>
            <td>""</td>

            <td>
              <p>
                Space separated list of URLs to use to publish the
                <a href="custom_scripts.html#logging">custom logs</a>.
              </p>
              <p>
                A token-authenticated Web server must be running at the given
                URL to receive the POST requests from the Glideins
              </p>
              <p>
                This attribute requires a <tt>factory upgrade</tt>. A
                <tt>reconfig</tt> is not sufficient because it changes the files
                transferred via HTCondor.
              </p>
            </td>
          </tr>
        </table>
      </div>
      <div class="section">
        <h3><a name="factory_vars"></a>Factory config xml - configuration</h3>
        <p>
          The second set of variables comes from values the Glidein Factory
          administrator defined to make the Factory to work. They are generated
          based on xml tags in the Factory configuration (most in the entry
          tag). They cannot be changed by an administrator in any other way.
        </p>
        <table class="attributes">
          <tr class="head">
            <td><b>Name</b></td>
            <td><b>Type</b></td>

            <td><b>Source</b></td>
            <td><b>Description</b></td>
          </tr>
          <tr>
            <td><b>GLIDEIN_Factory</b></td>

            <td>String</td>
            <td>&lt;glidein factory_name=&quot;<b>value</b>&quot;&gt;</td>
            <td>
              <p>
                Logical name of the Glidein Factory machine (like
                &ldquo;osg1&rdquo;).
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_Name</b></td>
            <td>String</td>
            <td>&lt;glidein glidein_name=&quot;<b>value</b>&quot;&gt;</td>
            <td>
              <p>
                Identification name of the Glidein Factory instance (like
                &ldquo;v1_0&rdquo;).
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_Entry_Name</b></td>
            <td>String</td>
            <td>
              ...&lt;entries&gt;&lt;entry name=&rdquo;<b>value</b>&rdquo;&gt;
            </td>
            <td>
              <p>
                Identification name of the entry point (like
                &ldquo;ucsd5&rdquo;).
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_GridType</b></td>

            <td>String</td>
            <td>
              ...&lt;entries&gt;&lt;entry
              gridtype=&ldquo;<b>value</b>&rdquo;&gt;
            </td>
            <td>
              <p>Type of Grid resource (like &ldquo;condor&rdquo;).</p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_Gatekeeper</b></td>
            <td>String</td>

            <td>
              ...&lt;entries&gt;&lt;entry
              gatekeeper=&ldquo;<b>value</b>&rdquo;&gt;
            </td>
            <td>
              <p>
                URI of the Grid gatekeeper (like
                &ldquo;osg1.ucsd.edu/jobmanager-pbs&rdquo;)
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_GlobusRSL</b></td>
            <td>String</td>
            <td>
              ...&lt;entries&gt;&lt;entry rsl=&ldquo;<b>value</b>&rdquo;&gt;
            </td>
            <td>
              <p>
                Optional RSL string (like &quot;(condor_submit=('+ProdSlot'
                'TRUE'))&quot;)
              </p>
            </td>
          </tr>
          <tr>
            <td><b>PROXY_URL</b></td>

            <td>String</td>
            <td>
              ...&lt;entries&gt;&lt;entry
              proxy_url=&ldquo;<b>value</b>&rdquo;&gt;
            </td>
            <td>
              <p>Optional URL of the site Web proxy.</p>
              <p>
                A special value &ldquo;OSG&rdquo; can be used to automatically
                discover the local Web proxy on OSG worker nodes.
              </p>
              <p>
                This variable is exported as <b>GLIDEIN_Proxy_URL</b> to the use
                job environment.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>DEBUG_MODE</b></td>
            <td>String</td>
            <td>
              ...&lt;entries&gt;&lt;entry
              verbosity=&ldquo;<b>value</b>&rdquo;&gt;
            </td>
            <td>
              <p>This setting can be either:</p>
              <ul>
                <li>
                  <p>
                    &ldquo;std&rdquo; - Default mode, where all interesting
                    debug information is reported back to the Glidein Factory
                    and the glidein will wait 20 minutes on a worker node that
                    failed validation to minimize the black hole effect.
                  </p>
                </li>

                <li>
                  <p>
                    &ldquo;nodebug&rdquo; - Disable most diagnostic messages.
                    This can be useful for very stable setups. The glidein still
                    waits 20 minutes on a worker node that failed validation to
                    minimize the black hole effect.
                  </p>
                </li>

                <li>
                  <p>
                    Fast &ndash; All debugging is enabled and the glidein waits
                    only 2 minutes on a worker node that failed validation. This
                    mode is useful when debugging a misbehaving Grid site.
                  </p>
                </li>
              </ul>
            </td>
          </tr>
        </table>
      </div>
      <div class="section">
        <h3>
          <a name="factory_frontend_attrs"></a>Factory or Frontend variables
          (config - attr tags)
        </h3>
        <p>
          This set of values may come from the Glidein Factory or Frontend
          configuration. While a client can set any number of variables, the
          ones described below are the most often used. The value set in the
          Factory can be overridden by the Frontend, unless set constant in the
          Factory. Some of the values below must be a parameter (i.e. have
          parameter="True" attr specification). This is because they are used in
          the early phases of the Glidein execution. NOTE that as of 3.4.6, in
          the Factory, also publish=True and const=False should be set,
          otherwise parameter=True will be ignored.
        </p>
        <table class="attributes">
          <tr class="head">
            <td><b>Name</b></td>

            <td><b>Type</b></td>
            <td><b>Description</b></td>
          </tr>
          <tr>
            <td><b>GLIDEIN_DEBUG_OUTPUT</b></td>

            <td>Str</td>
            <td>
              <p>
                If True enables debug output from the Glidein (more verbose
                messages). The messages printed are affected also by
                GLIDEIN_DEBUG_OPTIONS. If not empty is considered True.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_DEBUG_OPTIONS</b></td>

            <td>Str</td>
            <td>
              <p>
                Comma separated list of keywords affecting Glidein debug.
                Possible options are:<br />
                userjob - prints debug information on the user job stderr (at
                the beginning, when invoked with the container -Apptainer-
                wrapper)<br />
                usertrace - enable shell tracing (set -x) on the user job stderr
                (at the beginning, when invoked with the container -Apptainer-
                wrapper)<br />
                nowait - shortens the Glidein wait time to 2m when it ends with
                errors. Useful when debugging. The wait time protects against
                black holes<br />
                nocleanup - disable the cleanup (removal of working directories
                on the worker node). The batch manager may have its own cleanup.
                Adds also an extension to the Glidein directory name.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>MODULE_USE</b></td>

            <td>Expr (Bool)</td>
            <td>
              <p>
                Module and Spack provided by OSG on CVMFS are setup if this is
                true (1), the default. To avoid the setup set this to 0 (false).
                This can be set on the Factory, Frontend and in the Job submit
                file (+MODULE_USE), each overriding the previous ones in this
                list.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_MULTIGLIDEIN</b></td>
            <td>Int</td>
            <td>
              <p>
                Forks multiple glideins, each in its own subdirectory, all w/
                the same parameters (similar to multinode submission, but is all
                on one node). Unlsess GLIDEIN_MULTIGLIDEIN_LAUNCHALL is set,
                then a single invocation is used, expecting the system to handle
                the spawning. <b>Must be a parameter.</b>
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_MULTIGLIDEIN_LAUNCHER</b></td>
            <td>String</td>
            <td>
              <p>
                Used only if GLIDEIN_MULTIGLIDEIN is set and > 1, and
                GLIDEIN_MULTIGLIDEIN_LAUNCHALL is not set. Defines a launcher
                program that is added at the beginning of the command line when
                each multiple Glidein instance is started. NOTE: parameter and
                job_publish should be both True for this attribute.
                <b>Must be a parameter.</b>
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_MULTIGLIDEIN_LAUNCHALL</b></td>
            <td>String</td>
            <td>
              <p>
                Used only if GLIDEIN_MULTIGLIDEIN is set and > 1. When this is
                set, a command line with this string followed by the glidein
                invocation is used to start all Glideins. NOTE: parameter and
                job_publish should be both True for this attribute.
                <b>Must be a parameter.</b>
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_CLEANUP_SCRIPT</b></td>
            <td>String</td>
            <td>
              <p>
                Used to run a custom file at cleanup time (right before quitting
                the Glidein) instead of the default setup time. The custom file
                must be specified in the file list as usual. The value is the
                file name, as specified in the file list. NOTE: parameter and
                job_publish should be both True for this attribute.
                <b>Must be a parameter.</b>
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_CVMFS_REPOS</b></td>
            <td>String</td>
            <td>
              <p>
                Colon separated list of additional CVMFS repositories to mount
                via Glidein. NOTE: parameter and job_publish should be both True
                for this attribute, const should be False.
                <b>Must be a parameter.</b>
              </p>
            </td>
          </tr>
        </table>
      </div>

      <div class="section">
        <h3><a name="frontend_vars"></a>Frontend Client Variables</h3>
        <p>
          The third set of values comes from the Glidein Frontend clients. While
          a client can set any number of variables, the ones described below are
          the most often used.
        </p>
        <table class="attributes">
          <tr class="head">
            <td><b>Name</b></td>

            <td><b>Type</b></td>
            <td><b>Description</b></td>
          </tr>
          <tr>
            <td><b>GLIDEIN_Client</b></td>

            <td>String</td>
            <td>
              <p>
                Identification name of the VO Frontend request (like
                &ldquo;ucsd5@v1_0@osg1@cms4&rdquo;).
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_Collector</b></td>

            <td>Expr (List)</td>
            <td>
              <p>
                List of Collector URIs used by the VO Condor pool (like
                &ldquo;cc.cms.edu:9620,cc.cms.edu:9621&rdquo;).
              </p>
              <p>
                One of the URIs in the list will be selected and used as
                <b>HEAD_NODE</b> in the condor_config.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDECLIENT_Hold</b></td>
            <td>Expr (Bool)</td>

            <td rowspan="2">
              <p>
                Condor expression to use to specify when a user job in the
                glideins should be held. If any expression is true, the glidein
                is held. This is usually done to specify "bad" jobs, such as
                those that claim too much memory.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDECLIENT_Group_Hold</b></td>
            <td>Expr (Bool)</td>
          </tr>

          <tr>
            <td><b>GLIDECLIENT_PREEMPT</b></td>
            <td>Expr (Bool)</td>

            <td rowspan="2">
              <p>
                Condor expression to use to specify when a user job in the
                glideins should be preempted. If any expression is true, the
                glidein is preempted. This is usually done to specify custom
                preemption policies.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDECLIENT_Group_PREEMPT</b></td>
            <td>Exp (Bool)</td>
          </tr>

          <tr>
            <td><b>GLIDECLIENT_Rank</b></td>
            <td>Expr (Int)</td>
            <td rowspan="2">
              <p>
                Used in calculating the Condor
                <a
                  href="https://htcondor.readthedocs.io/en/latest/admin-manual/configuration-macros.html?highlight=NUM_CPUS#configuration-macros"
                  >RANK</a
                >
              </p>
              <p>
                They are summed together, and the user job with the largest rank
                will run first.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDECLIENT_Group_Rank</b></td>
            <td>Expr (Int)</td>
          </tr>
          <tr>
            <td><b>GLIDEIN_Ignore_X509_Duration</b></td>
            <td>Expr (Bool)</td>

            <td>
              <p>
                True by default. Setting this to false will shorten the Glidein
                lifetime to be shorter than the Proxy lifetime. Use it if you
                expect the resource will not extend reliably the proxy lifetime.
                The Glidein lifetime will always be shorten if the Proxy
                lifetime is less than 900 s.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_Job_Max_Time</b></td>
            <td>Int</td>
            <td>
              <p>
                Max allowed time for the job to end.<br />
                This is used by the condor_startd. Once the GLIDEIN_Retire_Time
                expires, the glidein will allow the job to run for this amount
                of time before activating DAEMON_SHUTDOWN and killing the job.
                Note that, if GLIDEIN_Max_Walltime is specified, this variable
                could be reduced by the glidein in order to fit within
                Glidein_Max_Walltime. See
                <a href="#lifetime">Lifetime of a glidein</a> for details.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_Expose_Grid_Env</b></td>
            <td>Expr (Bool)</td>

            <td>
              <p>
                If False, the user job environment will contain only glidein
                Factory provided variables.
              </p>
              <p>
                If True (default), the user job environment will also contain
                the environment variables defined at glidein startup.
              </p>
              <p>
                See
                <a
                  href="https://htcondor.readthedocs.io/en/latest/admin-manual/configuration-macros.html?highlight=JOB_INHERITS_STARTER_ENVIRONMENT#htcondor-wide-configuration-file-entries"
                  >JOB_INHERITS_STARTER_ENVIRONMENT documentation</a
                >
                for more details.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_Expose_X509</b></td>
            <td>Expr (Bool)</td>

            <td>
              <p>
                By default, the glidein will unset the variable X509_USER_PROXY
                for security reasons to prevent the user jobs from accessing the
                pilot proxy. Setting this to true will override this behavior
                and keep the X509_USER_PROXY in the environment.
              </p>
            </td>
          </tr>
          <tr></tr>
          <tr>
            <td><b>SLOTS_LAYOUT</b></td>
            <td>String</td>

            <td>
              Defines how multi-core glideins should split their resources.<br /><br />
              There are only two legal values:
              <ul>
                <li>
                  <b>fixed</b> - create N single core slots, and split memory
                  and disk equally mong them
                </li>
                <li>
                  <b>partitionable</b> - start up with
                  <a
                    href="https://htcondor.readthedocs.io/en/latest/admin-manual/policy-configuration.html?highlight=Policy%20configuration#policy-configuration-for-execute-hosts-and-for-submit-hosts"
                    >a single partitionable slot and let HTCondor do the
                    splitting as needed</a
                  >
                </li>
              </ul>

              The default behavior is defined by the Factory, but it has
              historically been set to "fixed".<br /><br />
              <b
                >Note: This variable MUST NOT be passed as a parameter, or the
                glideins will fail!</b
              >
            </td>
          </tr>
          <tr>
            <td><b>FORCE_PARTITIONABLE</b></td>
            <td>String (Bool)</td>

            <td>
              By default, single core glideins will never be configured as
              partitionable, independently of the value of
              <b>SLOTS_LAYOUT</b>.<br /><br />

              If partitionable slots are desired also for single-core glideins,
              set this variable to <b>"True"</b>.
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_Report_Failed</b></td>
            <td>String</td>

            <td>
              <p>
                This attribute regulates advertising of validation failures to
                the user collector.
              </p>
              <ul>
                <li>
                  <b>NEVER</b> - The glidein will just fail, and do nothing.
                  (default)
                </li>
                <li>
                  <b>ALIVEONLY</b> - The glidein will advertise a Machine
                  ClassAd in <i>Drained</i> state and <i>Retiring</i> activity
                  for the lifetime of the glidein script.
                </li>
                <li>
                  <b>ALWAYS</b> - Similar to <b>ALIVEONLY</b>, but will also
                  send a final ClassAd in <i>Drained</i> state and
                  <i>Killing</i> activity just before dying.
                </li>
              </ul>

              <p>
                When advertised, the classad is flagged
                <i>GLIDEIN_Failed=True</i>, the error is recorded in the
                <i>GLIDEIN_FAILURE_REASON</i> and
                <i>GLIDEIN_EXIT_CODE</i> attributes, and the failing script is
                recorded in the <i>GLIDEIN_LAST_SCRIPT</i> attribute.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_CLAIM_WORKLIFE</b></td>
            <td>Int</td>

            <td>
              <p>
                CLAIM_WORKLIFE for non-dynamic slots. Defaults to -1 i.e.
                HTCondor will treat this as an infinite claim worklife and
                schedd will hold claim to the slot until jobs are preempted or
                user runs out of jobs.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_CLAIM_WORKLIFE_DYNAMIC</b></td>
            <td>Int</td>

            <td>
              <p>
                CLAIM_WORKLIFE for dynamically partitionable slots. Defaults to
                3600. This controls how frequently the dynamically partionable
                slots will coalesce.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_Custom_Start</b></td>
            <td>Expr (Bool)</td>

            <td>
              <p>
                Defaults to true. An expression that gets appended to the start
                expression of the startd daemons that the pilots start on the
                worker node. Any frontend validation script can modify this and
                customize the set of user jobs that are matching. This is useful
                in order to have only a certain set of user jobs (e.g.: only
                montecarlo jobs) matching pilots at particular resources (e.g.:
                sites with slow network/disk)
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_BLACKHOLE_NUMJOBS</b></td>
            <td>Int</td>
            <td>
              <p>
                This attribute represents number of jobs completed too quickly
                before raising the black hole detection flag. More details in
                black hole detection section, on the bottom of who webpage
                (Lifetime of a Glidein). When defined, must be an integer bigger
                than 0.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_BLACKHOLE_RATE</b></td>
            <td>Float</td>
            <td>
              <p>
                Rate at which a Glidein is flagged as a black hole and retired.
                If a Glidein consumes more than GLIDEIN_BLACKHOLE_RATE jobs per
                second, then it will become a black hole, and stop accepting new
                jobs. This is a float value represented as string, i.e. string
                type for the shell, but used as floating point number in condor
                configuration. When undefined or set to 0.0, the black hole
                mechanism is disabled. See the blackhole detection section below
                (Lifetime of a Glidein) for more details.
              </p>
            </td>
          </tr>
          <tr>
            <td id="CONTINUE_IF_NO_PROXY"><b>CONTINUE_IF_NO_PROXY</b></td>
            <td>String (Bool)</td>

            <td>
              <p>
                Variable to control the transition from x509 to tokens. Defaults
                to false. When set to true, Factory entries with auth_method
                "grid_proxy" are used also when there is only a scitoken
                credential and no proxy. When False, the default, a proxy is
                required to submit to those entries, but if there is also a
                scitoken it is sent along together with the proxy.
              </p>
            </td>
          </tr>
        </table>
      </div>

      <div class="section">
        <h3><a name="vm_vars"></a>Cloud VM specific Variables</h3>
        <p>The following variables are only applicable to Cloud VMs.</p>
        <p>
          These variables can be either configured by Factory or the Frontend
        </p>
        <table class="attributes">
          <tr class="head">
            <td>
              <p><b>Name</b></p>
            </td>
            <td>
              <p><b>Type</b></p>
            </td>
            <td>
              <p><b>Default Value</b></p>
            </td>
            <td>
              <p><b>Description</b></p>
            </td>
          </tr>
          <tr>
            <td><b>VM_MAX_LIFETIME</b></td>
            <td>Int</td>
            <td>
              172800 <br />
              (48 hours)
            </td>
            <td>
              <p>
                Max lifetime of the VM. When this timer is reached the
                glideinwms-pilot service will terminate and glidein process,
                shutdown the glideinwms-pilot service and issue a VM shutdown
              </p>
            </td>
          </tr>
          <tr>
            <td><b>VM_DISABLE_SHUTDOWN</b></td>
            <td>Expr (Bool)</td>
            <td>False</td>
            <td>
              <p>
                Disables VM from automatically shutting down after the
                glideinwms-pilot service has exited.<br />
                <font color="ff0000"
                  ><b>NOTE:</b> This variable is for debugging purposes only and
                  should not be set to True in the Production environment.
                  Disabling auto shutdown could result in accumulating financial
                  charges in case of paid clouds.</font
                >
              </p>
            </td>
          </tr>
        </table>
      </div>

      <div class="section">
        <h3><a name="dyn_vars"></a>Dynamically generated variables</h3>
        <p>
          The following variables are being dynamically generated and/or
          modified by GlideinWMS processes. The GlideinWMS administrators cannot
          directly change them.
        </p>
        <p>The first set of variables comes from the Glidein Factory.</p>
        <table class="attributes">
          <tr class="head">
            <td><b>Name</b></td>

            <td><b>Type</b></td>
            <td><b>Description</b></td>
          </tr>
          <tr>
            <td><b>GLIDEIN_Signature</b></td>

            <td>String</td>
            <td rowspan="2">
              <p>
                These variables contain the SHA1 signature of the signature
                files.
              </p>
              <p>
                These signatures are used as a base to ensure the integrity of
                all the data downloaded in the glidein startup scripts, but they
                also provide a fingerprint of the configuration used by the
                glidein.
              </p>
              <p>
                These variables are published both in the glidein ClassAd and in
                the user job environenmt.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_Entry_Signature</b></td>
            <td>String</td>
          </tr>
          <tr>
            <td><b>CONDORG_SCHEDD</b></td>
            <td>String</td>
            <td>
              <p>
                The schedd used by the Glidein Factory to submit the glidein.
              </p>
              <p>
                This variables is exported a <b>GLIDEIN_Schedd </b>both in the
                glidein ClassAd and to the user job environment.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>CONDORG_CLUSTER</b></td>
            <td>Int</td>
            <td rowspan="2">
              <p>
                The cluster and process id assigned by the Glidein Factory
                schedd to this glidein.
              </p>
              <p>
                These variables are exported as <b>GLIDEIN_ClusterId</b> and
                <b>GLIDEIN_ProcId</b> both in the glidein ClassAd and to the
                user job environment.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>CONDORG_SUBCLUSTER</b></td>
            <td>Int</td>
          </tr>
          <tr>
            <td><b>GLIDEIN_BLACKHOLE</b></td>
            <td>Expr (Bool)</td>
            <td>
              <p>
                True if the Glidein has been detected as a black hole, False
                otherwise. The black hole condition is evaluated using
                GLIDEIN_BLACKHOLE_NUMJOBS, GLIDEIN_BLACKHOLE_RATE, and HTCondor
                statistics about jobs completing in the Glidein, and it affects
                the whole Glidein (all its static or partitionable slots). See
                the blackhole detection section below (Lifetime of a Glidein)
                for more details.
              </p>
            </td>
          </tr>
        </table>
      </div>
      <div class="section">
        <h3><a name="path_vars"></a>Directory Path Variables</h3>
        <p>
          The next set contains the location of files and/or directories
          downloaded or created by the glidein. Most of them are located under
          the working directory specified by
        </p>
        <blockquote>&lt;entry work_dir=&ldquo;value&rdquo;&gt;</blockquote>
        <table class="attributes">
          <tr class="head">
            <td><b>Name</b></td>
            <td><b>Description</b></td>
          </tr>
          <tr>
            <td><b>TMP_DIR</b></td>
            <td>
              <p>
                Path to the directory that admin-provided scripts and user jobs
                can use for storing temporary data.
              </p>
              <p>
                This variable is exported as <b>GLIDEIN_Tmp_Dir</b> both to the
                glidein ClassAd and to the user job environment.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_LOCAL_TMP_DIR</b></td>
            <td>
              <p>
                Path to the directory on the local file system for storing
                temporary data.<br />
                <b>Note:</b> Only use for things that absolutely need the
                guarantee of residing in the local file system. Prefer
                <b>TMP_DIR</b> above for everything else.
              </p>
              <p>This variable is exported to the user job environment.</p>
            </td>
          </tr>
          <tr>
            <td><b>CONDOR_VARS_FILE</b></td>
            <td rowspan="2">
              <p>File path to the condor vars files.</p>
              <p>
                Admin-provided scripts may want to add entries to these files.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>CONDOR_VARS_ENTRY_FILE</b></td>
          </tr>
          <tr>
            <td><b>ADD_CONFIG_LINE_SOURCE</b></td>

            <td>
              <p>
                File path to the script containing the add_config_line and
                add_confir_vars line functions.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>X509_USER_PROXY</b></td>
            <td>
              <p>File path to the glidein proxy file.</p>
            </td>
          </tr>
          <tr>
            <td><b>X509_CONDORMAP</b></td>
            <td>
              <p>File path to the Condor mapfile used by the glidein.</p>
            </td>
          </tr>
          <tr>
            <td><b>X509_CERT_DIR</b></td>
            <td>
              <p>
                Path to the directory containing the trusted CAs' public keys
                and RSLs.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>CONDOR_DIR</b></td>
            <td>
              <p>
                Directory where the glidein Condor binary distribution have been
                installed.
              </p>
            </td>
          </tr>

          <tr>
            <td><b>WRAPPER_LIST</b></td>
            <td>
              <p>
                File path to the list of wrapper scripts used by the glidein.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_WRAPPER_EXEC</b></td>
            <td>
              <p>
                This is the executable that glideins will run (ie what to put
                after "exec" in the condor job wrapper). By default, glideins
                will perform <tt>exec "\$@"</tt> to run the pilot. For other
                modes of execution, you may need different arguments. For
                example, to run a program under parrot, you may need
                <tt>exec "$GLIDEIN_PARROT/parrot_run" -t "$parrot_tmp" "$@" </tt
                >.<br />
                Note, the syntax of this command is very sensitive This needs to
                be double-escaped in order to function correctly. For the two
                examples listed above, you would need:
              </p>
              <ul>
                <li><tt>"\\\"\\\$@\\\""</tt></li>
                <li>
                  <tt
                    >"\\\"\\\$GLIDEIN_PARROT/parrot_run\\\" -t
                    \\\"$parrot_tmp\\\" \\\"\\\$@\\\""</tt
                  >
                </li>
              </ul>
            </td>
          </tr>
        </table>
      </div>
      <div class="section">
        <h3>
          <a name="glidein_vars"></a>Machine Job Features variables (dynamic)
        </h3>
        <p>
          This set of variables contains various variables generated by the
          glidein startup and periodic scripts, that are related to the
          <a href="http://hepsoftwarefoundation.org/notes/HSF-TN-2016-02.pdf"
            >Machine Job Features</a
          >. The content of these variables can only be a number, or an a string
          that does not contain double quote or backslash characters. Otherwise,
          the variable content will be set to the string "WrongFormat". For
          convenience the description of those variable is replicated here in
          this page.
        </p>
        <table class="attributes">
          <tr class="head">
            <td><b>Name</b></td>
            <td><b>Type</b></td>
            <td><b>Description</b></td>
          </tr>
          <tr>
            <td><b>MJF_MACHINE_TOTAL_CPU</b></td>
            <td>Int</td>
            <td>
              <p>
                Number of processors which may be allocated to jobs. Typically
                the number of processors seen by the operating system on one
                worker node (that is the number of \processor :" lines in
                /proc/cpuinfo on Linux), but potentially set to more or less
                than this for performance reasons.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>MJF_MACHINE_HS06</b></td>
            <td>Int</td>
            <td>
              <p>
                Total HS06 rating of the full machine in its current setup. HS06
                is measured following the HEPiX recommendations, with HS06
                benchmarks run in parallel, one for each processor which may be
                allocated to jobs.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>MJF_MACHINE_SHUTDOWNTIME</b></td>
            <td>Int</td>
            <td>
              <p>
                Shutdown time for the machine as a UNIX time stamp in seconds.
                The value is dynamic and optional. If the file is missing, no
                shutdown is foreseen.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>MJF_MACHINE_GRACE_SECS</b></td>
            <td>Int</td>
            <td>
              <p>
                If the resource provider announces a shutdown time to the jobs
                on this host, that time will not be less than grace secs seconds
                after the moment the shutdown time is set. This allows jobs to
                begin packages of work knowing that there will be sufficient
                time for them to be completed even if a shutdown time is
                announced. This value is required if a shutdown time will be set
                or changed which will affect any jobs which have already started
                on this host.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>MJF_JOB_ALLOCATED_CPU</b></td>
            <td>Int</td>
            <td>
              <p>Number of processors allocated to the current job.</p>
            </td>
          </tr>
          <tr>
            <td><b>MJF_JOB_HS06_JOB</b></td>
            <td>Int</td>
            <td>
              <p>
                Total HS06 rating for the processors allocated to this job. The
                job's share is calculated by the resource provider from
                per-processor HS06 measurements made for the machine.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>MJF_JOB_SHUTDOWNTIME_JOB</b></td>
            <td>Int</td>
            <td>
              <p>
                Dynamic value. Shutdown time as a UNIX time stamp in seconds. If
                the file is missing no job shutdown is foreseen. The job needs
                to have finished all of its processing when the shutdown time
                has arrived.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>MJF_JOB_GRACE_SECS_JOB</b></td>
            <td>Int</td>
            <td>
              <p>
                If the resource provider announces a shutdowntime job to the
                job, it will not be less than grace secs job seconds after the
                moment the shutdown time is set. This allows jobs to begin
                packages of work knowing that there will be sufficient time for
                them to be completed even if a shutdown time is announced. This
                value is static and required if a shutdown time will be set or
                changed after the job has started.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>MJF_JOB_JOBSTART_SECS</b></td>
            <td>Int</td>
            <td>
              <p>
                UNIX time stamp in seconds of the time when the job started on
                the worker node. For a pilot job scenario, this is when the
                batch system started the pilot job, not when the user payload
                started to run.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>MJF_JOB_ID</b></td>
            <td>Int</td>
            <td>
              <p>
                A string of printable non-whitespace ASCII characters used by
                the resource provider to identify the job at the site. In batch
                environments, this should simply be the job ID. In virtualized
                environments, job id will typically contain the UUID of the VM.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>MJF_JOB_WALL_LIMIT_SECS</b></td>
            <td>Int</td>
            <td>
              <p>
                Elapsed time limit in seconds, starting from jobstart secs. This
                is not scaled up for multiprocessor jobs.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>MJF_JOB_CPU_LIMIT_SECS</b></td>
            <td>Int</td>
            <td>
              <p>
                CPU time limit in seconds. For multiprocessor jobs this is the
                total for all processes started by the job
              </p>
            </td>
          </tr>
          <tr>
            <td><b>MJF_JOB_MAX_RSS_BYTES</b></td>
            <td>Int</td>
            <td>
              <p>
                Resident memory usage limit, if any, in bytes for all processes
                started by this job.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>MJF_JOB_MAX_SWAP_BYTES</b></td>
            <td>Int</td>
            <td>
              <p>
                Swap limit, if any, in bytes for all processes started by this
                job.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>MJF_JOB_SCRATCH_LIMIT_BYTES</b></td>
            <td>Int</td>
            <td>
              <p>
                Scratch space limit if any. If no quotas are used on a shared
                system, this corresponds to the full scratch space available to
                all jobs which run on the host. User jobs from EGI-registered
                VOs expect the \max size of scratch space used by jobs" value on
                their VO ID Card to be available to each job in the worst case.
                If there is a recognised procedure for informing the job of the
                location of the scratch space (eg EGI's $TMPDIR policy), then
                this value refers to that space.
              </p>
            </td>
          </tr>
        </table>
      </div>

      <div class="section">
        <h3>
          <a name="singularity_vars"></a>Apptainer/Singularity Variables
          (dynamic)
        </h3>

        <p>
          These variables can be set in the Factory attributes, set in the
          Frontend attributes, set in the Job (submit file), set in the Worker
          Node environment (Env), or cannot be set and is dynamically generated
          by the startup scripts (Dynamic). A value can be also be a combination
          of the above: the Factory value can be overridden by the Frontend one
          (unless set as constant), and the startup scripts can modify it and
          have the ultimate saying.
        </p>
        <p>
          The resulting invocation of Apptainer/Singularity is something like
          the text below. Where $singularity_binds is built using
          GLIDEIN_SINGULARITY_BINDPATH_DEFAULT, GLIDEIN_SINGULARITY_BINDPATH,
          and GWMS_SINGULARITY_BIND_CVMFS, and the $singularity_image is one
          coming from SINGULARITY_IMAGES_DICT or specified in the job. The debug
          options are used if GLIDEIN_DEBUG_OUTPUT is set.
        </p>
        <p>
          GlideinWMS is compatible also with Apptainer. Internal variables still
          use the "singularity" name but will set the proper environment
          variables for both programs and look for both executables.
        </p>
        <pre>
 singularity [-d -vvv] exec --home "$PWD":/srv --pwd /srv --ipc --pid --contain \
    $GLIDEIN_SINGULARITY_OPTS --bind "$singularity_binds"} \
    "$singularity_image" "${@}"
</pre
        >
        <p>
          The singularity binary (see singularity_lib.sh/singularity_locate_bin)
          is the first binary passing the test, looking in order in:
        </p>
        <ol>
          <li>
            If defined (in environment, Frontend or Factory): Try first the
            override binary or path (GLIDEIN_SINGULARITY_BINARY_OVERRIDE)
          </li>
          <li>
            If defined (in environment, Frontend or Factory): Try a singularity
            binary in the directory suggested via SINGULARITY_BIN: (keyword
            CONDOR -> apptainer in HTCondor tar ball, keyword OSG (default)->
            OSG_SINGULARITY_BINARY, keyword PATH -> go to the next step and
            check the system PATH)
          </li>
          <li>Try the singularity binary in PATH</li>
          <li>Try the apptainer binary in the HTCondor tar ball</li>
          <li>
            Try the singularity binary in PATH after invoking module
            singularitypro
          </li>
          <li>
            Try the singularity binary in PATH after invoking module singularity
          </li>
          <li>Try the default OSG location (OSG_SINGULARITY_BINARY)</li>
        </ol>

        <table class="attributes">
          <tr class="head">
            <td>
              <p><b>Name</b></p>
            </td>
            <td>
              <p><b>Type</b></p>
            </td>
            <td>
              <p><b>Default Value</b></p>
            </td>
            <td>
              <p><b>Where</b></p>
            </td>
            <td>
              <p><b>Description</b></p>
            </td>
          </tr>
          <tr>
            <td><b>CVMFS_MOUNT_DIR</b></td>
            <td>String (path)</td>
            <td></td>
            <td>Frontend Factory Dynamic</td>
            <td>
              <p>
                Not set by default. When set it indicates the path different
                from /cvmfs where CVMFS is mounted. Normally set by Glidein
                scripts when they mount CVMFS but cannot mount it on /cvmfs. A
                site administrator could set it in the environment or ask to set
                it in a Factory entry. It is not set inside Singularity because
                the external path can be mount internally to /cvmfs. To allow
                for more flexibility and uniformity across sites, paths like
                SINGULARITY_IMAGES_DICT, GLIDEIN_SINGULARITY_BINDPATH, ... can
                all use /cvmfs as prefix for their paths and it will be
                translated by the Glidein.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_SINGULARITY_REQUIRE</b></td>
            <td>String (Enum)</td>
            <td></td>
            <td>Factory</td>
            <td>
              <p>
                An entry can control the use of Singularity by setting
                GLIDEIN_SINGULARITY_REQUIRE to NEVER (Singularity is not
                supported), OPTIONAL or PREFERRED (capable of Singularity but it
                is not enforced), REQUIRED (jobs must run with Singularity) or
                REQUIRED_GWMS (jobs must run with Singularity and use the GWMS
                wrapper scripts). This last option is the only one that really
                enforces Singularity, but is not compatible with VOs that
                currently self-manage Singularity with custom scripts, like OSG
                and CMS.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_Singularity_Use</b></td>
            <td>String (Enum)</td>
            <td>DISABLE_GWMS</td>
            <td>Frontend</td>
            <td>
              <p>
                Determines whether or not Frontend/Group want to mandate the use
                of Singularity and is used both for provisioning and once the
                job is on the site. Possible values are: DISABLE_GWMS (disable
                GWMS Singularity machinery, not to interfere with independent VO
                setups - Default), or NEVER (do not use Singularity), or
                OPTIONAL or PREFERRED (use Singularity if the site is configured
                with it), or REQUIRED (always require Singularity). If REQUIRED,
                PREFERRED or OPTIONAL are used, the default wrapper script that
                we provide should be used (or a similar one). In other words a
                line like &lt;file
                absfname=&quot;<i>/var/lib/gwms-frontend/web-base/frontend/default_singularity_wrapper.sh</i>&quot;
                wrapper=&quot;True&quot;/&gt; should be added in the
                &lt;files&gt; section of the general or group parts of the
                Frontend configuration.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GWMS_SINGULARITY_STATUS</b></td>
            <td>String (Enum)</td>
            <td></td>
            <td>Dynamic</td>
            <td>
              <p>
                Is evaluated using GLIDEIN_Singularity_Use and
                GLIDEIN_SINGULARITY_REQUIRE and determines whether and how
                Singularity will be used. Possible values are: DISABLE (do not
                use the GWMS Singularity mechanisms), FAIL (do not provision on
                this entry, do not match with these Glideins or fail the
                Glidein), NEVER (do not use Singularity), PREFERRED (use
                Singularity but allow to fall-back to no Singularity if
                something goes wrong, e.g. no image is found), REQUIRED (use
                Singularity and fail the Glidein if it cannot run with
                Singularity).<br />
                GWMS_SINGULARITY_STATUS_EFFECTIVE, if set, will override
                GWMS_SINGULARITY_STATUS, e.g. for a specific job.<br />
                See the
                <a href="../factory/configuration.html#singularity"
                  >Factory configuration document</a
                >
                for a table of how Singularity is calculated using
                GLIDEIN_Singularity_Use and GLIDEIN_SINGULARITY_REQUIRE.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GWMS_SINGULARITY_STATUS_EFFECTIVE</b></td>
            <td>String (Enum,String)</td>
            <td></td>
            <td>Dynamic</td>
            <td>
              <p>
                Is used to override GWMS_SINGULARITY_STATUS in special cases,
                e.g. Set to "REQUIRED_Singularity_image_in_job" to disable
                fall-back when SingularityImage is in the Job.<br />
                See also GWMS_SINGULARITY_STATUS.<br />
                It is composed by the status, followed by underscore and a
                string explaining the override.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>[GWMS_]SINGULARITY_MODE</b></td>
            <td>String (Enum)</td>
            <td></td>
            <td>Dynamic</td>
            <td>
              <p>
                Is calculated if a valid singularity binary is found. Possible
                values are (see the Singularity documentation for the meaning of
                privileged/unprivileged): privileged, unprivileged, fakeroot
                (unprivileged mode with user mapped internally to uid 0). A
                string &quot;singularity/GWMS_SINGULARITY_MODE&quot; is also
                added to GLIDEIN_PROVIDES. There are other dynamic variables set
                when Singularity is found: [GWMS_]SINGULARITY_PATH,
                [GWMS_]SINGULARITY_VERSION.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>HAS_SINGULARITY</b></td>
            <td>Expr (Bool)</td>
            <td></td>
            <td>Dynamic</td>
            <td>
              <p>
                Tells the Glidein whether Singularity is available. This is not
                set if GWMS_SINGULARITY_STATUS is DISABLE, it is set to 1 (true)
                if GWMS_SINGULARITY_STATUS is PREFERRED or REQUIRED, and it is
                set to 0 (false) otherwise. It can change to 0 if
                GWMS_SINGULARITY_STATUS is PREFERRED but the Singularity tests
                fail.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>SINGULARITY_IMAGES_DICT</b></td>
            <td>String</td>
            <td></td>
            <td>Frontend Factory</td>
            <td>
              <p>
                String-encoded comma separated key-value pair dictionary listing
                Singularity images. Keys cannot contain commas or colons. Values
                cannot contain commas. The keys are identifiers for a platform
                and the values are paths (or URLs) for the corresponding images,
                expressed in any format supported by Singularity. REQUIRED_OS is
                used to select the platform: the first platform in the list that
                matches a key will be used. You can use any string to identify
                the images as long as you are consistent with REQUIRED_OS, but
                we recommend to use standard names like: rhel9 (for RHEL9 or
                AlmaLinux9), rhel7 (for RHEL7 or SL7). The default set in
                generic_pre_singularity_setup.sh is
                "rhel7:/cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-el7:latest,rhel6:/cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-el6:latest,rhel8:/cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-el8:latest"
                or by running a pre_singularity setup script.
                SINGULARITY_IMAGE_DEFAULT6, SINGULARITY_IMAGE_DEFAULT7, and
                SINGULARITY_IMAGE_DEFAULT are <b>deprecated</b> and will be
                ignored in the future. To ease the transition, these are
                currently added to SINGULARITY_IMAGES_DICT with keys rhel6,
                rhel7 and default, respectively.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>REQUIRED_OS</b></td>
            <td>String</td>
            <td>any</td>
            <td>Frontend Job</td>
            <td>
              <p>
                Comma separated list of platforms accepted by the job. 'any' is
                a wildcard, means that any platform is OK. GWMS will intersect
                this list with GLIDEIN_REQUIRED_OS and use the intersection to
                select the Singularity image. The first value of the
                intersection for which there is a key in SINGULARITY_IMAGES_DICT
                will determine the image (value with that key). 'any' means that
                any Singularity image is OK.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_REQUIRED_OS</b></td>
            <td>String</td>
            <td>any</td>
            <td>Factory</td>
            <td>
              <p>
                Comma separated list of platforms accepted by the entry. See
                REQUIRED_OS.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>SINGULARITY_IMAGE_RESTRICTIONS</b></td>
            <td>String (Enum)</td>
            <td>any</td>
            <td>Factory Frontend</td>
            <td>
              <p>
                Comma separated list of restrictions on the Singularity images.
                Possible values are: cvmfs (the image must be on cvmfs); remote
                (it is OK to use remote images via URI); test (if no valid image
                is found, return the test image and not error).
              </p>
            </td>
          </tr>
          <tr>
            <td><b>SINGULARITY_IMAGE_REQUIRED</b></td>
            <td>String (Enum)</td>
            <td>false</td>
            <td>Factory Frontend</td>
            <td>
              <p>
                If "false" the Glidein will continue the Singularity/Apptainer
                setup even if no valid image is found. The setup will use a test
                image (see APPTAINER_TEST_IMAGE). The user will have to provide
                the image with a later setup script or in the job. Possible
                values are: false or true.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>APPTAINER_TEST_IMAGE</b></td>
            <td>String (Enum)</td>
            <td>Not set</td>
            <td>Factory Frontend CE</td>
            <td>
              <p>
                URI (local path or remote URI) of a Singularity/Apptainer image
                to use for testing if your image is not available and
                GWMS_SINGULARITY_IMAGE_DEFAULT is not set. APPTAINER_TEST_IMAGE
                can be set in the worker node environment, which takes
                precedence, or as attribute in the configuration. If neither is
                set, the Glidein will download for testing
                <tt>oras://ghcr.io/apptainer/alpine:latest</tt>, a small Alpine
                Linux sif image provided by the Apptainer developers.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_SINGULARITY_BINDPATH_DEFAULT</b></td>
            <td>String</td>
            <td></td>
            <td>Factory</td>
            <td>
              <p>
                This is used to set Singularity bind mounts. It is a comma
                separated list of src[:dst[:opt]] elements. See
                GLIDEIN_SINGULARITY_BINDPATH
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_SINGULARITY_BINDPATH</b></td>
            <td>String</td>
            <td></td>
            <td>Frontend</td>
            <td>
              <p>
                This is used to set Singularity bind mounts. It is a comma
                separated list of <tt>src[:dst[:opt]]</tt> elements, as you
                would express in the --bind option in Singularity. 'src' is the
                path on the system, 'dst' is the path in the Singularity image,
                'opt' is a mount option (e.g. 'ro', for read-only mounts). See
                the Singularity for a description of the format and the
                supported options. Commas are not allowed in both the paths, and
                colons are not allowed in the 'src' path. GWMS combines in order
                GLIDEIN_SINGULARITY_BINDPATH,
                GLIDEIN_SINGULARITY_BINDPATH_DEFAULT, and a default list. The
                default list includes
                "/hadoop,/hdfs,/lizard,/mnt/hadoop,/mnt/hdfs", and "/cvmfs" (if
                GWMS_SINGULARITY_BIND_CVMFS is true) and some GPUs libraries (if
                GPUs are detected). If CVMFS is mounted in a different place,
                CVMFS_MOUNT_DIR will be set in the environment and the Glidein
                will replace /cvmfs with the correct mount point
                ($CVMFS_MOUNT_DIR:/cvmfs), so that inside Singularity CVMFS will
                always be available in /cvmfs, even if outside that is not
                possible. The combined list is validated removing paths not
                existing (outside Singularity) and is passed as --bind option to
                singularity. Singularity prepends to the mounts list also the
                environment variable $SINGULARITY_BINDPATH (this could be set by
                the Site or by setup scripts on the worker nodes). Singularity
                will skip mounts where the destination path does not exist in
                the Singularity image selected. The suggestion is to set
                GLIDEIN_SINGULARITY_BINDPATH_DEFAULT in the Factory and
                GLIDEIN_SINGULARITY_BINDPATH in the Frontend.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GWMS_SINGULARITY_BIND_CVMFS</b></td>
            <td>Expr (Bool)</td>
            <td>1</td>
            <td>Frontend Factory</td>
            <td>
              <p>
                CVMFS (/cvmfs) is mounted in Singularity if this is 1 (true)
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_SINGULARITY_OPTS</b></td>
            <td>String</td>
            <td></td>
            <td>Frontend Factory</td>
            <td>
              <p>
                Extra options that will be added to the singularity invocation
              </p>
            </td>
          </tr>
          <tr>
            <td><b>SINGULARITY_DISABLE_PID_NAMESPACES</b></td>
            <td>String</td>
            <td></td>
            <td>Frontend Factory</td>
            <td>
              <p>
                When set, it disables the <t>--pid</t> option in the
                Singularity/Apptainer invocation, which is added by default.
                This is needed when running inside Docker or Podman (e.g. in
                conteinarized worker nodes).
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_SINGULARITY_BINARY_OVERRIDE</b></td>
            <td>String</td>
            <td></td>
            <td>Env Frontend Factory</td>
            <td>
              <p>
                Path singularity binary (full path of the executable OR one or
                more directories separated by colons, ":"). If provided, this is
                the first path searched by the Glidein. If set as attribute make
                sure that <i>job_publish</i> is set to True.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>OSG_SINGULARITY_BINARY</b></td>
            <td>String</td>
            <td></td>
            <td>Frontend Factory</td>
            <td>
              <p>
                Path in CVMFS of the OSG provided singularity binary. This is
                one of the paths searched by the Glidein. By default
                /cvmfs/oasis.opensciencegrid.org/mis/singularity/bin/singularity
              </p>
            </td>
          </tr>
          <tr>
            <td><b>SINGULARITY_BIN</b></td>
            <td>String</td>
            <td></td>
            <td>Frontend Factory</td>
            <td>
              <p>
                If set to "OSG" or not set (default), the Glidein will try first
                to use the singularity binary provided by OSG in CVMFS before
                looking in the system PATH and trying module. If set to
                "CONDOR", the Glidein will try first to use the apptainer binary
                provided by the HTCondor tar ball. If set to "PATH", the Glidein
                will try first to use the singularity binary in the system PATH.
                SINGULARITY_BIN can be used also to suggest an arbitrary
                specific folder for the singularity binary that will be tested
                (and used) before the other options, but this use is only for
                debug purposes, not recommended in production, not to overload
                Factory Ops with tracking site changes. NOTE: in pre-v3.4
                versions SINGULARITY_BIN was used also to enable the use of
                Singularity for a Factory entry. This is no more, use
                GLIDEIN_SINGULARITY_REQUIRE instead.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>APPTAINER_CACHEDIR</b> <b>SINGULARITY_CACHEDIR</b></td>
            <td>String</td>
            <td>$GLIDEIN_LOCAL_TMP_DIR/apptainer/cache</td>
            <td>Frontend Factory</td>
            <td>
              <p>
                Set to control where Apptainer and singularity place their cache
                directory, default (only if the variable is unset, not if it is
                set and empty) is $GLIDEIN_LOCAL_TMP_DIR/apptainer/cache. Note
                that you can set any Apptainer/Singularity variable to affect
                their behavior.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>APPTAINER_CACHEDIR</b> <b>SINGULARITY_CACHEDIR</b></td>
            <td>String</td>
            <td>$GLIDEIN_LOCAL_TMP_DIR/apptainer/tmp</td>
            <td>Frontend Factory</td>
            <td>
              <p>
                Set to control where Apptainer and singularity place their temp
                directory, default (only if the variable is unset, not if it is
                set and empty) is $GLIDEIN_LOCAL_TMP_DIR/apptainer/tmp. If set
                to null (""), Apptainer will search in /tmp and $TMPDIR.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_CONTAINER_ENV</b></td>
            <td>String</td>
            <td></td>
            <td>Frontend Factory</td>
            <td>
              <p>
                Controls which environment variables will be cleared or restored
                in Singularity. Can have a comma separated list (no spaces) of
                the following values: <br />
                clearall - clear all from the environment unless other sets are
                listed (clearing everything may cause problems also w/ some
                Glidein functionalities);<br />
                condorset - preserves HTCondor variables (env variables starting
                with _CONDOR_ and variables defined in the job ClassAd
                (Environment)<br />
                osgset - preserve the set of OSG variables (originally from OSG
                wrapper);<br />
                gwmsset - preserve the set of GWMS utilities variables;<br />
                clear - same as: clearall,gwmsset,osgset,condorset;<br />
                clearpaths - clear only PATH,LD_LIBRARY_PATH,PYTHONPATH
                (default);<br />
                keepall - clear no variable, incompatible w/ any clear...
                option<br />
                condorset and osgset both imply also gwmsset. osgset implies
                also condorset.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_CONTAINER_ENV_CLEARLIST</b></td>
            <td>String</td>
            <td></td>
            <td>Frontend Factory</td>
            <td>
              <p>
                Comma separated list of environment variable names that should
                be cleared before starting the container
              </p>
            </td>
          </tr>
        </table>
      </div>

      <div class="section">
        <h2><a name="cvmfs_vars"></a>CVMFS Variables</h2>
        <p>
          GlideinWMS factory includes the capability to mount and unmount CVMFS
          on demand, i.e. for sites that do not have a native installation of
          CVMFS available. <b>cvmfs_setup.sh</b> and <b>cvmfs_umount.sh</b> are
          the main scripts used for mounting and unmounting CVMFS. A couple of
          auxiliary scripts serve as helper scripts for different purposes: (1)
          <i>cvmfs_helper_funcs.sh</i> helps with on-demand CVMFS provisioning,
          and (2) <i>cvmfsexec_platform_select.sh</i> assists with dynamic
          selection of cvmfsexec distribution. On-demand mounting and unmounting
          is achieved via the
          <b>
            <i
              ><a href="https://www.github.com/cvmfs/cvmfsexec">cvmfsexec</a></i
            >
          </b>
          utility, that supports various modes as outlined
          <a
            href="https://github.com/cvmfs/cvmfsexec?tab=readme-ov-file#cvmfsexec-package"
            >here</a
          >.
        </p>
        <p>
          The table below describes the attributes that can be configured for
          use with the glidein to allow CVMFS to be provisioned on-demand. These
          variables are configured at the glidein level so that it applies to
          all the entries that are configured in the factory's configuration
          file.
        </p>
        <table class="attributes">
          <tr class="head">
            <td>
              <p><b>Name</b></p>
            </td>
            <td>
              <p><b>Type</b></p>
            </td>
            <td>
              <p><b>Default Value</b></p>
            </td>
            <td>
              <p><b>Where</b></p>
            </td>
            <td>
              <p><b>Description</b></p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_USE_CVMFSEXEC</b></td>
            <td>Int</td>
            <td></td>
            <td>Factory</td>
            <td>
              <p>
                Not set by default. This variable controls whether
                <i>cvmfsexec</i> utility is used to mount CVMFS. Possible values
                are 0 or 1. A value of 0 indicates that glidein will not use
                <i>cvmfsexec</i> to mount CVMFS on the worker noden. A value of
                1 indicates <i>cvmfsexec</i> will be used to mount CVMFS on the
                worker node by the glidein.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>CVMFS_SRC</b></td>
            <td>String</td>
            <td></td>
            <td>Factory</td>
            <td>
              <p>
                Not set by default. This variable controls the origin (source)
                from which to download the CVMFS configuration repository from.
                Possible values are: <br />
                osg - configuration for cvmfs-config-osg; <br />
                egi - configuration for cvmfs-config-egi; <br />
                default - configuration for cvmfs-config-default<br />
              </p>
            </td>
          </tr>
          <tr>
            <td><b>GLIDEIN_CVMFS</b></td>
            <td>String</td>
            <td></td>
            <td>Factory</td>
            <td>
              <p>
                Not set by default. This variable controls the error handling
                behavior of the glidein in case of failures during the mounting
                of CVMFS. Possible values are:<br />
                REQUIRED - if unable to mount, report an error and abort
                mounting);<br />
                PREFERRED - if unable to mount, notify and continue normally;<br />
                OPTIONAL - continue if unable to mount (similar to
                PREFERRED);<br />
                NEVER - only exit if mounting fails<br />
              </p>
            </td>
          </tr>
        </table>
        <br />
        <p>
          Following is a matrix summarizing the combination of requirements that
          allow different modes of cvmfsexec. For more technical details on how
          unprivileged user namespaces and FUSE dictate the mode of cvmfsexec
          that will be used, please refer to cvmfsexec documentation
          <a href="https://www.github.com/cvmfs/cvmfsexec">here</a>.
        </p>
        <table class="requirements">
          <tr class="head">
            <td><b>GLIDEIN_USE_CVMFSEXEC</b></td>
            <td><b>GLIDEIN_CVMFS</b></td>
            <td><b>Unprivileged user namespaces?</b></td>
            <td><b>FUSE/FUSE3 installed?</b></td>
            <td><b>cvmfsexec mode used</b></td>
            <td><b>Utility used?</b></td>
          </tr>
          <tr>
            <td>0</td>
            <td>NEVER|PREFERRED|OPTIONAL|REQUIRED</td>
            <td>N/A</td>
            <td>N/A</td>
            <td>N/A</td>
            <td>N/A</td>
          </tr>
          <tr>
            <td>1</td>
            <td>NEVER</td>
            <td>N/A</td>
            <td>N/A</td>
            <td>N/A</td>
            <td>N/A</td>
          </tr>
          <tr>
            <td rowspan="2">1</td>
            <td rowspan="2">PREFERRED|OPTIONAL</td>
            <td>no</td>
            <td>yes</td>
            <td>1</td>
            <td>mountrepo</td>
          </tr>
          <tr>
            <td>yes</td>
            <td>N/A</td>
            <td>3</td>
            <td>cvmfsexec</td>
          </tr>
          <tr>
            <td rowspan="3">1</td>
            <td rowspan="3">REQUIRED</td>
            <td>yes</td>
            <td>N/A</td>
            <td>3</td>
            <td>cvmfsexec</td>
          </tr>
          <tr>
            <td rowspan="2">no</td>
            <td>yes</td>
            <td>1</td>
            <td>mountrepo</td>
          </tr>
          <tr>
            <td>no</td>
            <td>-</td>
            <td>-</td>
          </tr>
        </table>
        <br /><br />
        <h4>
          Dynamic creation and selection of cvmfsexec platform-specific
          distribution
        </h4>
        <p>
          <i>cvmfsexec</i> can be packaged as self-contained distribution
          specific to an operating system and platform with required
          configuration and executables necessary to enable CVMFS in
          unprivileged mode. Doing so allows easy distribution to multiple
          machines. Worker node specifications vary from each other and can
          change over time so, shipping all possible variations of cvmfsexec
          distributions with the glidein helps address the challenge with using
          the correct distribution for use by the worker node. However, rather
          than having a tarball containing multiple cvmfsexec distribution files
          corresponding to a (CVMFS source, system platform, architecture)
          combination, GlideinWMS dynamically creates and selects the
          appropriate cvmfsexec distribution from the list of available
          distributions.
        </p>
        <p>
          Specific to a CVMFS source, system platform and architecture,
          cvmfsexec distribution files are created automatically by the script
          named <b>create_cvmfsexec_distros.sh</b>, which is available under
          `/usr/bin` as an executable and is invoked dynamically during factory
          reconfiguration and/or upgrade when on-demand CVMFS is enabled. Each
          of these distributions are packaged as individual tarballs and are
          added to the default list of uploads at the time of factory
          reconfiguration and/or upgrade. During glidein startup, the auxiliary
          script <i>cvmfsexec_platform_select.sh</i>, automatically selects the
          appropriate distribution tarball based on the specifics of the worker
          node and ships the distribution with the glidein. This script is
          invoked during the customization of the worker node as part of the
          glidein setup.
        </p>
        <p>
          Since <i>create_cvmfsexec_distros.sh</i> is executed as part of
          factory reconfiguration and/or upgrade, there might be times when the
          dynamic building of cvmfsexec distributions fails. For instance, a
          misspelling exists in `platforms` attribute value. Instead of
          reinvoking factory reconfiguration and/or upgrade in such cases, as an
          alternative, <i>create_cvmfsexec_distros.sh</i> can be executed
          manually following the required syntax as described in the script's
          usage information (via `create_cvmfsexec_distros.sh -h`) to retry the
          building of cvmfsexec distributions that had previously failed.
        </p>
      </div>

      <div class="section">
        <h3><a name="glidein_vars"></a>Glidein Script Variables (dynamic)</h3>

        <p>
          The last set contains various variables generated by the glidein
          startup scripts.
        </p>
        <table class="attributes">
          <tr class="head">
            <td><b>Name</b></td>
            <td><b>Type</b></td>
            <td><b>Description</b></td>
          </tr>
          <tr>
            <td><b>X509_GRIDMAP_DNS</b></td>
            <td>String</td>
            <td>
              <p>List of DNs trusted by the glidein.</p>
            </td>
          </tr>
          <tr>
            <td><b>X509_EXPIRE</b></td>
            <td>Expr (time_t)</td>
            <td>
              <p>When is the proxy expected to expire.</p>
            </td>
          </tr>
          <tr>
            <td><b>SiteWMS_WN_Draining</b></td>
            <td>Expr (Bool)</td>

            <td>
              <p>
                The variable controls whether the glidein should accept new jobs
                or not. As part of the
                <a
                  href="https://twiki.cern.ch/twiki/bin/view/LCG/MachineJobFeatures"
                >
                  WLCG Machine / Job Features Task Force</a
                >
                Site admins have the possibility to put worker nodes to "drain
                mode". In particular if the JOBFEATURES (or MACHINEFEATURES)
                environment variable is set and it points to a local directory
                containing a file named
                <tt>shutdowntime_job</tt> (<tt>shutdowntime</tt>), or
                <tt>$JOBFEATURES/shutdowntime_job</tt>
                (<tt>$MACHINEFEATURES/shutdowntime</tt>) is a valid URL (it is
                possible to wget it), then the glidein will stop accepting jobs.
                (see
                <a
                  href="https://twiki.cern.ch/twiki/bin/view/LCG/WMTEGEnvironmentVariables"
                >
                  here</a
                >). The <tt>SiteWMS_WN_Draining</tt> variable is periodically
                updated by means of the STARTD_CRON condor feature. See
                <a href="#draining">Draining glideins</a> for details.
              </p>
            </td>
          </tr>
          <tr>
            <td><b>SiteWMS_WN_Preempt</b></td>
            <td>Expr (Bool)</td>

            <td>
              <p>
                The variable controls whether the glidein should preempt jobs or
                not. This is still part of the
                <a
                  href="https://twiki.cern.ch/twiki/bin/view/LCG/MachineJobFeatures"
                >
                  WLCG Machine / Job Features Task Force</a
                >
                (see above). The variable will become true if the shutdown time
                value contained in the file pointed by the JOBFEATURES (or
                MACHINEFEATURES) variable contains a timestamp (unix epoch time)
                that is less than 30 minutes in the future. If so
                <tt>PREEMPT_GRACE_TIME</tt> will be set to 20 minutes and the
                job will be preempted after that time if it does not exit. The
                <tt>SiteWMS_WN_Preempt</tt> variable is periodically updated by
                means of the STARTD_CRON condor feature. See
                <a href="#draining">Draining glideins</a> for details.
              </p>
            </td>
          </tr>
        </table>
      </div>
      <div class="section" id="cpuscalculation">
        <h2>Set and discover available CPUs</h2>
        <p>
          For this explanation the term CPUs refers to the number of cores,
          provided by one or more CPUs. It is whatever is detected or provided
          and will set the level of parallelism (NUM_CPUS) used by the HTCondor
          startd started by the Glidein.
        </p>
        <p>Jobs express the number of CPUs they require via RequestCpus.</p>
        <p>
          You can force HTCondor to assume it has a certain number of CPUs by
          setting GLIDEIN_CPUS to a number or let the Glidein discover the
          number of physical CPUs in the worker node (<tt>node</tt>) or the
          number of CPUs received from the job manager (<tt>slot</tt>,
          <tt>auto</tt>).
        </p>
        <p>
          If GLIDEIN_CPUS is set to <tt>auto</tt> (or <tt>slot</tt>,
          <tt>node</tt>). When the Frontend does provisioning, it triggers
          requests to resources where RequestCpus&lt;=GLIDEIN_ESTIMATED_CPUS (1
          if GLIDEIN_ESTIMATED_CPUS is not set) and it is dimensioning the
          request expecting to receive CPUs = GLIDEIN_ESTIMATED_CPUS *
          number_of_glidiens.
          <br />
          When Glideins actually start at the site, they advertise the CPUs they
          actually discover, ignoring what is set in GLIDEIN_ESTIMATED_CPUS and
          will match jobs with RequestCpus&lt;=CPUs discovered. This is
          different from setting GLIDEIN_CPUS, that artificially tells HTCondor
          in the Glidein that it has those CPUs, over- or under-provisioning if
          the number of CPUs discovered is different.
        </p>
        <p>
          So if you have a site with a variable number of CPUs you can set
          GLIDEIN_ESTIMATED_CPUS to the min number of CPUs you will be receiving
          (for ease call it MinCpus). All jobs triggering Glideins on that site
          will be able to run on them. Multiple jobs will run on the same
          Glidein if it receives more CPUs. If you need to run jobs w/
          RequestCpus&gt;MinCpus, you can set GLIDEIN_ESTIMATED_CPUS to a bigger
          number but be aware that some Glideins (the ones that received less
          than GLIDEIN_ESTIMATED_CPUS CPUs) will not match jobs that triggered
          their submission (those Glideins will probably be wasted).
        </p>
        <p>
          Another attribute changing the number of CPUs is
          GLIDEIN_Resource_Slots. When you add resource slots of type
          <tt>extra</tt>, <tt>partitionable</tt> or <tt>static</tt>, these
          increase the number of CPUs set in HTCondor, independently from
          whether the initial number was set with GLIDEIN_CPUS or discovered.
          These virtual CPUs are though not considered in provisioning, are
          frequently reserved for jobs matching the special resources
          (<tt>partitionable</tt>, <tt>static</tt>) and are available also for
          regular jobs only when the slot type <tt>extra</tt>
          is used.
        </p>
        <p>
          For more information, see the descriptions of GLIDEIN_CPUS,
          GLIDEIN_ESTIMATED_CPUS, and GLIDEIN_Resource_Slots above. And
          <a
            href="https://htcondor.readthedocs.io/en/latest/admin-manual/configuration-macros.html?highlight=NUM_CPUS#condor-starter-configuration-file-entries"
            >NUM_CPUS</a
          >
          in the HTCondor manual.
        </p>
        <p>
          Other variables related to resource definition are GLIDEIN_DISK,
          GLIDEIN_MaxMemMBs, and GLIDEIN_MaxMemMBs_Estimate. See above.
        </p>
      </div>

      <div class="section" id="lifetime">
        <h2>Lifetime of a Glidein</h2>
        <img src="../images/retire_times.png" alt="Retire time plot" />
        <p>
          All the various variables in the glidein configuration can be
          confusing. The above diagram illustrates the lifetime of a glidein
          pilot that has a long-running job.
        </p>
        <ul>
          <li>
            <font color="499946"><i>Green</i></font
            >: For the first GLIDEIN_Retire_Time seconds (modified by a random
            spread GLIDEIN_Retire_Spread to smooth out glideins all ending
            simultaneously), jobs can start on the glidein pilot.
          </li>
          <li>
            <font color="f8d844"><i>Yellow</i></font
            >: During the yellow period, START will evaluate to FALSE, so no new
            jobs will start. However, already running jobs will continue to run
            for GLIDEIN_Job_Max_Time. (Note that the glideins will end during
            this period if the job ends. They will not idle afterwards since no
            new jobs can start anyway).
          </li>
          <li>
            Once this period is done, the DAEMON_SHUTDOWN variable will be true.
          </li>
          <li>
            <font color="f8bb44"><i>Orange</i></font
            >: There can be a delay of up to UPDATE_INTERVAL (usually about 5
            minutes) between when DAEMON_SHUTDOWN becomes true and when it is
            actually updated in the collector. This is because the collector
            only reevaluates this expression periodically.
          </li>
          <li>
            <font color="c7484c"><i>Red</i></font
            >: Once DAEMON_SHUTDOWN is true, condor gives a short grace period
            of GLIDEIN_Graceful_Shutdown before forcefully terminating
            everything and shutting down. All of these periods are totalled and
            calculated to fit within GLIDEIN_MAX_WALLTIME if specified.
          </li>
        </ul>
        <p>
          Note that if a job ends early in the green period, a new job will
          start. If a job ends after this period, then the glidein will shut
          down and end early. This can be seen in the example below with two
          jobs below:
        </p>
        <img src="../images/idle_times.jpg" alt="Idle time plot" />
        <p></p>
        <ul>
          <li>
            <font color="f8bb44"><i>Orange</i></font
            >: A glidein first starts up.
          </li>
          <li>
            <font color="f8d844"><i>Yellow</i></font
            >: After starting up, the glidein will wait for Glidein_Max_Idle for
            its initial matching. If it is Idle for longer than this, it will
            assume no jobs are available and will shutdown.
          </li>
          <li>
            <font color="499946"><i>Green</i></font
            >: A job runs (subject to the maximum limits in the previous
            diagram).
          </li>

          <li>
            <font color="f8d844"><i>Yellow</i></font
            >: Once a job runs, it will wait for Glidein_Max_Tail for another
            job.
          </li>
          <li>
            <font color="499946"><i>Green</i></font
            >: Another matching job runs.
          </li>
          <li>
            <font color="f8d844"><i>Yellow</i></font
            >: Once the job finishes, it will wait for Glidein_Max_Tail for
            another job. If cannot find one, it will shutdown.
          </li>
        </ul>
        Glidein_Max_Tail counts the idle time after the previous job ends, as
        specified above. N.B.: 12 hours after the walltime, a glidein is removed
        from the factory queue no matter what is using a periodic remove
        expression. This is done to prevent <i>staleness of glideins</i> which
        are the glideins that appears running in the factory condor queue
        indefinitely, but are no longer in the CE queue.

        <p>
          Note: This information above applies when normal execution. When a
          black hole is detected, START will evaluate to False, meaning that no
          new jobs will start and the glidein will eventually retire. For more
          details, check out the black hole detection section, on the bottom of
          this webpage.
        </p>
      </div>

      <div class="section" id="draining">
        <h2>Draining glideins</h2>

        <p>
          In this section we describe the steps to follow to activate the
          feature that allows to drain glideins.
        </p>
        <ul>
          <li>
            On the execute hosts, create a directory in which MACHINEFEATURES
            attributes will be set, say its name is <tt>/path/to/dir</tt>.
          </li>
          <li>
            Set the MACHINEFEATURES job environment variable to point to the
            directory. For example, you can use the following condor
            configuration parameter:
            <tt>STARTER_JOB_ENVIRONMENT = "MACHINEFEATURES=/path/to/dir"</tt>
          </li>
          <li>
            When a machine is draining, create a file named "shutdowntime" in
            the MJF directory. Put the draining deadline unix timestamp in the
            file. Even a distant time in the future is sufficient to cause the
            glidein to set SiteWMS_WN_Draining=True and stop accepting new jobs
            (this is controlled by the <tt>check_wn_drainstate.sh</tt> condor
            CROND script).
          </li>
        </ul>
      </div>

      <div class="section">
        <a name="blackhole" />
        <h2>Draining glideins</h2>
        <p>
          Sometimes a worker node passes all the tests, the Glidein starts OK
          but all jobs keep failing for some internal reason (for example,
          something VO specific, not tested by the tests). In order to identify
          this case, the Frontend operator can now set up two variables which
          will help to recognise this event. Whenever too many jobs complete too
          quickly and both variables are matched, the Glidein will retire and
          add all the preventive measures to avoid black hole effects (e.g.
          holding the node for an additional 20 minutes) like when tests are
          failing. The two attributes are:
        </p>
        <ul>
          <li>
            GLIDEIN_BLACKHOLE_NUMJOBS (int): Number of jobs limit allowed to
            complete very quickly. The glidein will be flagged as a black hole
            when corresponding stats are equal to or greater than this variable.
          </li>
          <li>
            GLIDEIN_BLACKHOLE_RATE (int): Number of jobs divided by time - in
            seconds, to be completed. The Glidein will be flagged as a black
            hole when corresponding stats are lower than
            1/GLIDEIN_BLACKHOLE_RATE.
          </li>
          <p>
            We strongly recommend to set these two attributes to True
            (parameter, glidein_publish and job_publish). This way both
            attributes will be available in the condor_startd and glideclient
            ClassAd, in the glidein condor_configuration and in the user job's
            environment. Moreover, there is an internal (and dynamic) variable
            named GLIDEIN_BLACKHOLE (boolean), which will be TRUE when it detect
            that stats from the STARTD beat these two variables as explained
            previously. For more information, check the condor vars list. The
            stats are coming from two probes that will generate 16 attributes in
            each slot ad when enabled (which is a lot), so they are not
            published by default. We enable them for publication by adding this
            to the configuration of execute nodes (condor_startup.sh). The
            values are available immediately if the schedd is queried directly
            (START expression, direct query from client or startd cron script).
            The start expression is updated every time that job starts. Note
            that the mentioned probes were introduced in HTCondor 8.7.8, so make
            sure you have that version or newer in order to get the STARTD
            stats.
          </p>
        </ul>
      </div>

      <div class="footer">
        Banner image by
        <a href="http://www.flickr.com/people/leafwarbler/">Madhusudan Katti</a>
        used under Creative Commons license.<br />
        Original Home URL:
        <a href="https://glideinwms.fnal.gov">https://glideinwms.fnal.gov</a>.
        GlideinWMS email support: glideinwms-support at fnal.gov
      </div>
    </div>
  </body>
</html>
